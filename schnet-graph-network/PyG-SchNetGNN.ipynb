{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction of Molecular Properties using SchNet on Graphcore IPUs\n",
    "=================================================================\n",
    "> *Note: PyTorch Geometric support with PopTorch SDK 3.1 is currently experimental.*\n",
    "> *Please direct any questions or feedback to support@graphcore.ai*\n",
    "\n",
    "\n",
    "This notebook demonstrates training a [SchNet graph neural network](https://arxiv.org/abs/1712.06113) with PyTorch Geometric on the Graphcore IPU.  We will use the QM9 dataset from the [MoleculeNet: A Benchmark for Molecular\n",
    "    Machine Learning](https://arxiv.org/abs/1703.00564) paper and train the SchNet model to predict the HOMO-LUMO energy gap.\n",
    "\n",
    "This notebook assumes some familiarity with PopTorch as well as PyTorch Geometric (PyG).  For additional resources please consult:\n",
    "\n",
    "* [PopTorch Documentation](https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/index.html)\n",
    "* [PopTorch Examples and Tutorials](https://docs.graphcore.ai/en/latest/examples.html#pytorch)\n",
    "* [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running on Paperspace\n",
    "\n",
    "The Paperspace environment lets you run this notebook with no set up. To improve your experience we preload datasets and pre-install packages, this can take a few minutes, if you experience errors immediately after starting a session please try restarting the kernel before contacting support. If a problem persists or you want to give us feedback on the content of this notebook, please reach out to through our community of developers using our [slack channel](graphcorecommunity.slack.com) or raise a [GitHub issue](https://github.com/gradient-ai/Graphcore-Pytorch/issues).\n",
    "\n",
    "Requirements:\n",
    "\n",
    "* Python packages installed with `pip install -r requirements-pyg.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --force-reinstall  \"pyg-nightly==2.2.0.dev20221208\"\n",
    "%pip install -q -r requirements-pyg.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import poptorch\n",
    "import pandas as pd\n",
    "import py3Dmol\n",
    "\n",
    "from periodictable import elements\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import to_fixed_size\n",
    "from torch_geometric.nn.models import SchNet\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pyg_schnet_util import (TrainingModule, KNNInteractionGraph, prepare_data,\n",
    "                             padding_graph, create_dataloader, optimize_popart)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "poptorch.setLogLevel('ERR')\n",
    "executable_cache_dir = os.getenv(\"POPLAR_EXECUTABLE_CACHE_DIR\", \"/tmp/exe_cache/\") + \"/pyg-schnet\"\n",
    "dataset_directory = os.getenv(\"DATASET_DIR\", 'data')\n",
    "num_ipus = os.getenv(\"NUM_AVAILABLE_IPU\", \"16\")\n",
    "num_ipus = min(int(num_ipus), 16) # QM9 is too small to benefit from additional scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QM9 Dataset\n",
    "\n",
    "PyG provides a convenient dataset class that manages downloading the QM9 dataset to local storage. The QM9 dataset contains 130831 molecules with a number of different physical properties that we can train the SchNet model to predict.  For SchNet, a molecule with $n$ atoms is described by:\n",
    "\n",
    "* Nuclear charges $Z= (Z_1, Z_2, ..., Z_n)$, stored as a vector of integers of length `num_atoms`\n",
    "* Atomic positions $\\mathbf{r} = (\\mathbf{r}_1, \\mathbf{r}_2, \\ldots, \\mathbf{r}_n)$, stored as a tensor of real numbers of size `[num_atoms, 3]`\n",
    "\n",
    "We consider each molecule as an undirected graph where:\n",
    "* the atoms are the nodes or vertices of the graph.\n",
    "* the edges are inferred from the atomic positions by connecting atoms that are within a given cutoff radius to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9_root = osp.join(dataset_directory, 'qm9')\n",
    "dataset = QM9(qm9_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call `len` to see how many molecules are in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect each molecule which is represented as an instance of a [torch_geometric.data.Data](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.Data) object.  The properties we are interested in for training our SchNet model are:\n",
    "* `z` contains the atomic number for each atom in the molecule.\n",
    "* `pos` contains the 3d structure of the molecule.\n",
    "* `y` contains the 19 regression targets.  The HOMO-LUMO gap is stored in the 4th column so can be accessed by slicing this tensor using `y[:,4]`\n",
    "\n",
    "Next we display the first example molecule from the QM9 dataset as a `Data` object, the atomic number tensor, the positions tensor, and slice the regression targets to get the HOMO-LUMO gap for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datum = dataset[123244]\n",
    "datum, datum.z, datum.pos, datum.y[:, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a transform to the QM9 dataset we can select the properties we need for training SchNet and convert them to a PyG Data object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = prepare_data\n",
    "dataset[123244]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the [py3Dmol](https://github.com/3dmol/3Dmol.js/tree/master/packages/py3Dmol) package to visualise the molecule to get a better idea of the structure.  To do this we need to provide the simple `xyz` format to the `py3Dmol.view` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_atoms = int(datum.z.numel())\n",
    "xyz = f\"{num_atoms}\\n\\n\"\n",
    "\n",
    "for i in range(num_atoms):\n",
    "    sym = elements[datum.z[i].item()].symbol\n",
    "    r = datum.pos[i, :].tolist()\n",
    "    line = [sym] + [f\"{i: 0.08f}\" for i in r]\n",
    "    line = \"\\t\".join(line)\n",
    "    xyz += f\"{line}\\n\"\n",
    "\n",
    "view = py3Dmol.view(data=xyz, style={'stick': {}})\n",
    "view.spin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we collect some statistics by iterating over the entire dataset and investigate the distribution of the number of atoms in each molecule and the HOMO-LUMO gap energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_mols = len(dataset)\n",
    "num_atoms = []\n",
    "hl_gap = []\n",
    "\n",
    "for mol in tqdm(dataset):\n",
    "    num_atoms.append(mol.z.numel())\n",
    "    hl_gap.append(float(mol.y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pandas dataframe from the collected statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Number of atoms': num_atoms, 'HOMO-LUMO Gap (eV)': hl_gap})\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure shows how the number of atoms varies across the QM9 dataset as well as the kernel density estimate (KDE) of the HOMO-LUMO gap energy.The following histogram shows how the number of atoms varies across the QM9 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = plt.figure(figsize=[10, 4])\n",
    "sns.histplot(data=df, x=df.columns[0], ax=plt.subplot(1, 2, 1), discrete=True)\n",
    "sns.kdeplot(data=df, x=df.columns[1], ax=plt.subplot(1, 2, 2))\n",
    "h.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Batching\n",
    "\n",
    "PyG provides a specialized version of the native PyTorch [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html):\n",
    "\n",
    "* [torch_geometric.data.DataLoader](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.DataLoader)\n",
    "\n",
    "\n",
    "The PyG dataloader supports a form of mini-batching which is [decribed here](https://pytorch-geometric.readthedocs.io/en/latest/notes/batching.html).  Effectively each mini-batch is a concatenation of multiple graphs (molecules in QM9).  Another way to understand this is that each mini-batch is one large graph comprised of multiple disconnected sub-graphs.  The PyG dataloader will generate a `batch` vector that assigns each feature in the mini-batch into a distinct subgraph.  This is useful for message passing networks (such as SchNet) and pooling layers to produce a distinct regression prediction for each molecule. Refer to the following tutorials for additional background:\n",
    "\n",
    "* [Creating message passing networks](https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html)\n",
    "* [Global Pooling Layers](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html?highlight=pooling#global-pooling-layers)\n",
    "\n",
    "This mini-batching approach needs to be adapted for the IPU since the tensor sizes will vary from batch to batch.  This can be observed in the following cell where tensors such as `pos`, `z`, and `batch` all have different sizes between the first two batches of the QM9 dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=4)\n",
    "\n",
    "it = iter(loader)\n",
    "next(it), next(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SchNet Model Architecture\n",
    "\n",
    "![SchNet Architecture](./static_resources/schnet_arch.png \"SchNet Architecture\")\n",
    "\n",
    "The diagram above demonstrates the overall architecture of the SchNet model.  The main inputs to the model are:\n",
    "* $(Z_1, Z_2, \\ldots, Z_n)$ : A vector of atomic numbers which are used as input to the atom-wise embedding layer.\n",
    "* $(\\mathbf{r}_1, \\mathbf{r}_2, \\ldots, \\mathbf{r}_n)$: An `[n, 3]` tensor of atomic positions.\n",
    "\n",
    "The graph is defined by considering each atom as a node and the edges are defined by:\n",
    "* placing a sphere of radius $r_\\textrm{cut}$ centered on each atom.\n",
    "* connecting neighbouring atoms that fall within the cutoff sphere with an undirected edge.\n",
    "\n",
    "The rationale for using this cutoff sphere is to bound the maximum number of atoms that are connected to each other so that the computational cost grows linearly with the number of atoms in the system.\n",
    "\n",
    "By default the inter-atomic interaction graph will be computed using the `radius_graph` [method](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.pool.radius_graph) in PyTorch Geometric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IPU implementation of SchNet\n",
    "\n",
    "General support for PyTorch on the IPU is accomplished through ahead-of-time compilation with PopTorch. The compiler performs static analysis over the input tensors and the computational graph to optimise the evaluation on the IPU.  To fully leverage these optimisations for the SchNet architecture we need to enforce consistent tensors sizes for all:\n",
    "* operations used within the model.\n",
    "* mini-batches of molecular graphs that are inputs to the model.\n",
    "\n",
    "We first identify that the default interaction graph method using `radius_graph` will by definition create a variable number of edges depending on the geometric structure of the molecule.  This is unfriendly for the ahead-of-time compilation in PopTorch but we can reformulate the interaction graph as a k-nearest neighbours search.  We use the `interaction_graph` argument to the PyTorch Geometric SchNet [implementation](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.models.SchNet) to compute the pairwise interaction graph and interatomic distances.\n",
    "\n",
    "We can use a simple strategy of appending a padding graph to effectively fill up each mini-batch up to a known maximum possible size.  To accomplish this we need to define non-interacting padding atoms.  These padding atoms are defined as having atomic charge zero.  This ensures there are no artificial interactions introduced between these padding atoms and any real atoms within the mini-batch.\n",
    "\n",
    "Refer `pyg_schnet_util.py` file for the implementation details that are needed to fully realise an efficient evaulation of the SchNet GNN on the IPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a basic sanity check we can compile the SchNet model with PopTorch and check that we get the same prediction as running the model on the host CPU.\n",
    "\n",
    "Prepare a mock batch consisting of a single graph using the PyG `Batch.from_data_list` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = Batch.from_data_list([dataset[0]])\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the network on the CPU with randomly initialised weights using a fixed random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "cutoff = 10.0\n",
    "model = SchNet(cutoff=cutoff)\n",
    "model.eval()\n",
    "cpu = model(batch.z, batch.pos, batch.batch)\n",
    "cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate the network on the IPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "knn_graph = KNNInteractionGraph(cutoff=cutoff, k=batch.num_nodes - 1)\n",
    "model = SchNet(cutoff=cutoff, interaction_graph=knn_graph)\n",
    "model = to_fixed_size(model, batch_size=1)\n",
    "options = poptorch.Options()\n",
    "options.enableExecutableCaching(executable_cache_dir)\n",
    "pop_model = poptorch.inferenceModel(model, options)\n",
    "ipu = pop_model(batch.z, batch.pos, batch.batch)\n",
    "\n",
    "ipu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions must be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(cpu, ipu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "The easiest way to get up and running on the IPU with the QM9 dataset is just to apply padding to the input tensors. This results in every example in the dataset being expanded up to the size of the largest molecule.  This expansion comes at the cost of additional padding nodes and edges.\n",
    "\n",
    "We use the `PadMolecule` [transform](https://pytorch-geometric.readthedocs.io/en/latest/modules/transforms.html).  This transform modifies each `Data` instance in the dataset to have `max_num_atoms` nodes.\n",
    "\n",
    "The `PadMolecule` transform is defined in `pyg_schnet_util.py` along with a function that builds an entire data pre-processing pipeline.\n",
    "\n",
    "We can explore how the pipeline converts an input molecule into one that is padded up to the maximum graph size in the dataset.  An experiment to try is to change the value of `i` to explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[0]\n",
    "batch = Batch.from_data_list([data, padding_graph(32 - data.num_nodes)])\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next sanity check is to verify that the padding hasn't introduced any numerical artifacts in the resulting prediction.  Once again we prepare a mock batch consisting of a single graph but apply the transform we made earlier by calling `create_transform`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the network on the host with randomly initialised weights using a fixed random seed and the padded batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "model = SchNet(cutoff=cutoff)\n",
    "model.eval()\n",
    "padded_cpu = model(batch.z, batch.pos, batch.batch)\n",
    "padded_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result should be the same as the one we calculated earlier without any padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(cpu, padded_cpu[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate the same test using the IPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "knn_graph = KNNInteractionGraph(cutoff=cutoff, k=batch.num_nodes - 1)\n",
    "model = SchNet(cutoff=cutoff, interaction_graph=knn_graph)\n",
    "model = to_fixed_size(model, batch_size=2)\n",
    "pop_model = poptorch.inferenceModel(model, options)\n",
    "padded_ipu = pop_model(batch.z, batch.pos, batch.batch)\n",
    "\n",
    "padded_ipu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions must be the same as calculated earlier without any paddding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(ipu, padded_ipu[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detach the inference model from the IPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_model.detachFromDevice()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient data loading for the IPU\n",
    "\n",
    "PopTorch provides a custom data loader implementation that can be used for efficient data batching and transfers between the host and IPU device.  Please refer to the following resources for additional background:\n",
    "* PopTorch documentation [Efficient data batching](https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/batching.html#efficient-data-batching)\n",
    "* PopTorch tutorial: [Efficient data loading](https://github.com/graphcore/tutorials/tree/sdk-release-2.5/tutorials/pytorch/tut2_efficient_data_loading)\n",
    "\n",
    "Below we define a custom collater that leverages the PyG graph batching for the IPU.  This collator ensures that advanced batching scenarios such as data-parallel training, multiple device iterations, and gradient accumulation are handled correctly. These concepts are all covered in much greater detail in the resources above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a helper function that creates an instance of `poptorch.DataLoader` that uses the collator defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together to train SchNet\n",
    "\n",
    "We can now train SchNet on the IPU using all of the concepts introduced earlier.  To start with we shuffle and split the dataset into testing, validation, and training splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test = 10000\n",
    "num_val = 10000\n",
    "torch.manual_seed(0)\n",
    "dataset.transform = prepare_data\n",
    "dataset = dataset.shuffle()\n",
    "test_dataset = dataset[:num_test]\n",
    "val_dataset = dataset[num_test:num_test + num_val]\n",
    "train_dataset = dataset[num_test + num_val:]\n",
    "\n",
    "print(f\"Number of test molecules: {len(test_dataset)}\\n\"\n",
    "      f\"Number of validation molecules: {len(val_dataset)}\\n\"\n",
    "      f\"Number of training molecules: {len(train_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the hyperparameters for training the network.  These can be changed to explore the different trade-offs they offer in terms of training accuracy and performance throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "replication_factor = num_ipus\n",
    "device_iterations = 32\n",
    "gradient_accumulation = 16 // num_ipus\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the `poptorch.Options` object with the right parameters setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = poptorch.Options()\n",
    "options.enableExecutableCaching(executable_cache_dir)\n",
    "options.outputMode(poptorch.OutputMode.All)\n",
    "options.deviceIterations(device_iterations)\n",
    "options.replicationFactor(replication_factor)\n",
    "options.Training.gradientAccumulation(gradient_accumulation);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply a few additional options that can help improve performance for SchNet.  These optimisations are covered in greater detail in [Extreme Acceleration of Graph Neural Network-based Prediction Models for Quantum Chemistry](https://arxiv.org/abs/2211.13853).  For the purpose of this notebook you can experiment with changing the `additional_optimizations` variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_optimizations = True\n",
    "\n",
    "if additional_optimizations:\n",
    "    options = optimize_popart(options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the SchNet model and pre-compile for the IPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_dataloader(train_dataset,\n",
    "                                 options,\n",
    "                                 batch_size,\n",
    "                                 shuffle=True)\n",
    "torch.manual_seed(0)\n",
    "knn_graph = KNNInteractionGraph(cutoff=cutoff, k=28)\n",
    "model = SchNet(cutoff=cutoff, interaction_graph=knn_graph)\n",
    "model.train()\n",
    "model = TrainingModule(model,\n",
    "                       batch_size=batch_size,\n",
    "                       replace_softplus=additional_optimizations)\n",
    "optimizer = poptorch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "training_model = poptorch.trainingModel(model, options, optimizer)\n",
    "\n",
    "data = next(iter(train_loader))\n",
    "training_model.compile(*data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model with the selected hyperparameters and log the mean loss from each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    bar = tqdm(train_loader)\n",
    "    for i, data in enumerate(bar):\n",
    "        _, mini_batch_loss = training_model(*data)\n",
    "        loss = float(mini_batch_loss.mean())\n",
    "        train.append({'epoch': epoch, 'step': i, 'loss': loss})\n",
    "        bar.set_description(f\"Epoch {epoch} loss: {loss:0.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detach the training model from the IPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model.detachFromDevice()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the mean of the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(train)\n",
    "g = sns.lineplot(data=df[df.epoch > 0], x='epoch', y='loss', errorbar='sd')\n",
    "g.set_xticks(range(0, num_epochs+2, 2))\n",
    "g.figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow up\n",
    "The training loss looks like it is descreasing nicely over a relatively small number of epochs, try measuring the validation accuracy.  The following publications demonstrate using IPUs to train SchNet\n",
    "\n",
    "* [Reducing Down(stream)time: Pretraining Molecular GNNs using Heterogeneous AI Accelerators](https://arxiv.org/abs/2211.04598)\n",
    "* [Extreme Acceleration of Graph Neural Network-based Prediction Models for Quantum Chemistry](https://arxiv.org/abs/2211.13853)\n",
    "\n",
    "The dataset used in these papers is available in PyG as [HydroNet](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.HydroNet).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "9409b80169a82c0207afe9a460d7f88a38094a708839df55a31910312ecdb1ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
