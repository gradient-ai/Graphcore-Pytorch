{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation with GPT-J 6B\n",
    "\n",
    "[GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B) is a causal decoder-only transformer model which can be used for text-generation.\n",
    "Causal means that a causal mask is used in the decoder attention, so that each token has visibility on previous tokens only.\n",
    "\n",
    "Language models are very powerful because a huge variety of tasks can be formulated as a text-to-text problem and thus adapted to fit the generative setup, where the model is asked to correctly predict future tokens. This idea has been widely explored in [T5 paper: Exploring the Limits of Transfer Learning with a Unified\n",
    "Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf).\n",
    "\n",
    "In 5 lines of code, we show how GPT-J can be run on the Graphcore IPU and used to complete arbitrary NLP tasks using examples and structured prompting.\n",
    "For more complex tasks we show the benefit of fine-tuning and load a checkpoint from the Hugging Face Hub which achieves much better performance on the specific task.\n",
    "\n",
    "While no finetuning is performed in this notebook, you can find out how to fine-tune the model for your own dataset in the [finetuning notebook](finetuning.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "In order to run this notebook you will need to be in an environment with the Poplar SDK and PopART installed and enabled.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure smooth execution of the notebook, we load and check environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "number_of_ipus = int(os.getenv(\"NUM_AVAILABLE_IPU\", 16))\n",
    "if number_of_ipus < 16:\n",
    "    raise ValueError(\"This notebook is designed to run with at least 16 IPUs\")\n",
    "\n",
    "executable_cache_dir = os.getenv(\"POPLAR_EXECUTABLE_CACHE_DIR\", \"./exe_cache/\")\n",
    "os.environ[\"POPART_CACHE_DIR\"] = executable_cache_dir\n",
    "checkpoint_directory = os.getenv(\"CHECKPOINT_DIR\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running GPT-J on the IPU\n",
    "\n",
    "This notebook demonstrate an interactive interface to GPT-J which can be used to do text generation from arbitrary prompts.\n",
    "While this application implements GPT-J in Graphcore's PopXL framework, no knowledge of the framework is required to use or train this application as all parameters are controlled through configuration options.\n",
    "\n",
    " <!-- PopXL is a framework which provides fine grained control of execution, memory and parallelism.  -->\n",
    "Several model configurations are available in the `config/` folder. They can be loaded as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-15 17:22:03 INFO: Starting. Process id: 4054544\n",
      "checkpoint:\n",
      "  load: null\n",
      "  optim_state: true\n",
      "  save: null\n",
      "  steps: 0\n",
      "  to_keep: 4\n",
      "execution:\n",
      "  attention_serialisation: 1\n",
      "  available_memory_proportion:\n",
      "  - 0.4\n",
      "  code_load: false\n",
      "  data_parallel: 1\n",
      "  device_iterations: 1\n",
      "  io_tiles: 1\n",
      "  loss_scaling: 1\n",
      "  micro_batch_size: 16\n",
      "  tensor_parallel: 16\n",
      "inference:\n",
      "  output_length: 5\n",
      "model:\n",
      "  attention:\n",
      "    heads: 16\n",
      "    rotary_dim: 64\n",
      "    rotary_positional_embeddings_base: 10000\n",
      "  dropout_prob: 0.0\n",
      "  embedding:\n",
      "    vocab_size: 50400\n",
      "  eval: true\n",
      "  hidden_size: 4096\n",
      "  layers: 28\n",
      "  precision: float16\n",
      "  seed: 42\n",
      "  sequence_length: 1024\n",
      "training:\n",
      "  global_batch_size: 32\n",
      "  optimizer:\n",
      "    beta1: 0.9\n",
      "    beta2: 0.999\n",
      "    gradient_clipping: 1.0\n",
      "    learning_rate:\n",
      "      maximum: 0.01\n",
      "      warmup_proportion: 0.0\n",
      "    name: adamw\n",
      "    weight_decay: 0.01\n",
      "  steps: 1\n",
      "  stochastic_rounding: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Setup ---\n",
    "import run_inference\n",
    "\n",
    "config, *_ = run_inference.gptj_config_setup(\n",
    "    \"config/inference.yml\", \"release\", \"gpt-j-mnli\"\n",
    ")\n",
    "print(config.dumps_yaml())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This configuration object can be edited and stored in a new file to suit your needs. It contains all the arguments which control the execution of the application on the IPU and might need to be tuned to ensure the best performance for the application you are looking to deploy.\n",
    "\n",
    "Next we're going to combine this IPU configuration with pre-trained weights. The `pipeline` utility accepts directly the name of a pre-trained checkpoint from the Hugging Face Hub.\n",
    "In this case we are loading the [6 billion parameter GPT-J checkpoint from EleutherAI](https://huggingface.co/EleutherAI/gpt-j-6B), this checkpoint is trained on [the Pile](https://pile.eleuther.ai/) open source dataset.\n",
    "\n",
    "This checkpoint is a general language modelling checkpoint and has not been fine-tuned on a specific task.\n",
    "\n",
    "We create a `IPUGPTJPipeline`, in this first step the weights are downloaded from the Hugging Face Hub and a PopXL session is created, this initial step takes a few minutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-15 17:22:08 INFO: Creating session\n",
      "2023-02-15 17:22:08 INFO: Starting PopXL IR construction\n",
      "2023-02-15 17:22:35 INFO: PopXL IR construction duration: 0.44 mins\n",
      "2023-02-15 17:22:35 INFO: Starting PopXL compilation\n",
      "2023-02-15 17:22:38 INFO: PopXL compilation duration: 0.05 mins\n",
      "2023-02-15 17:22:38 INFO: Downloading 'EleutherAI/gpt-j-6B' pretrained weights and tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15T17:22:36.112930Z popart:popart 4054544.4054544 W: [Ir::setIsPrepared] setIsPrepared was already called. It should only be called once.\n",
      "WARNING: The compile time engine option debug.branchRecordTile is set to \"23551\" when creating the Engine. (At compile-tile it was set to 1471)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-15 17:23:55 INFO: Starting Loading HF pretrained model to IPU\n",
      "2023-02-15 17:25:13 INFO: Loading HF pretrained model to IPU duration: 1.31 mins\n"
     ]
    }
   ],
   "source": [
    "from utils import pipeline\n",
    "\n",
    "general_model = pipeline.IPUGPTJPipeline(\n",
    "    config,\n",
    "    \"EleutherAI/gpt-j-6B\",\n",
    "    sequence_length=256,\n",
    "    print_live=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attributes of the `general_model` can be explored:\n",
    "\n",
    "- `model` contains the `GPTJForCausalLM` from the Transformers library which is used to load the weights,\n",
    "- `tokenizer` contains the tokenizer loaded with the pre-trained checkpoint from the Hugging Face Hub,\n",
    "- `config` has the input config,\n",
    "- `session` is the PopXL session which controls the IPU executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='EleutherAI/gpt-j-6B', vocab_size=50257, model_max_len=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '<|extratoken_1|>'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_model.tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline class can be used to do standard text generation, here we ask it a simple questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-15 17:29:10 WARNING: Parameter 'function'=<function encode_for_inference at 0x7efff2650b80> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a1bb31335a4c00aca0899e5004a518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-15 17:29:10 INFO: Attach to IPUs\n",
      "2023-02-15 17:31:10 INFO: Start inference\n",
      "Prompt: 'What is the capital of France?'\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "What is the capital of Germany?\n",
      "\n",
      "The capital of Germany is Berlin.\n",
      "\n",
      "What is the capital of Italy?\n",
      "\n",
      "The capital of Italy is Rome.\n",
      "\n",
      "What is the capital of Spain?\n",
      "\n",
      "The capital of Spain is Madrid.\n",
      "\n",
      "What is the capital of the United Kingdom?\n",
      "\n",
      "The capital of the United Kingdom is London.\n",
      "\n",
      "What is the capital of the United States?\n",
      "\n",
      "The capital of the United States is Washington, D.C.\n",
      "\n",
      "What is the capital of Canada?\n",
      "\n",
      "The capital of Canada is Ottawa"
     ]
    }
   ],
   "source": [
    "out = general_model(\"What is the capital of France?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using prompt structure for improved results\n",
    "\n",
    "While it does get the correct answer it includes it in a long form answer and continues generating similar questions and answers.\n",
    "To refine this format, we can provide in the prompt a structure to our answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d4d9ff70fd4ac2a6fe1f4d62770dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-15 17:32:11 INFO: Attach to IPUs\n",
      "2023-02-15 17:32:11 INFO: Start inference\n",
      "Prompt: 'Question: What is the capital of Country?\n",
      "Answer: City\n",
      "Question: What is the capital of France?\n",
      "Answer:' Paris\n",
      "Question: What is the capital of Germany?\n",
      "Answer: Berlin\n",
      "Question: What is the capital of Italy?\n",
      "Answer: Rome\n",
      "Question: What is the capital of Spain?\n",
      "Answer: Madrid\n",
      "Question: What is the capital of the United States?\n",
      "Answer: Washington\n",
      "Question: What is the capital of the United Kingdom?\n",
      "Answer: London\n",
      "Question: What is the capital of Australia?\n",
      "Answer: Canberra\n",
      "Question: What is the capital of New Zealand?\n",
      "Answer: Wellington\n",
      "Question: What is the capital of Canada?\n",
      "Answer: Ottawa\n",
      "Question: What is the capital of Japan?"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' Paris\\nQuestion: What is the capital of Germany?\\nAnswer: Berlin\\nQuestion: What is the capital of Italy?\\nAnswer: Rome\\nQuestion: What is the capital of Spain?\\nAnswer: Madrid\\nQuestion: What is the capital of the United States?\\nAnswer: Washington\\nQuestion: What is the capital of the United Kingdom?\\nAnswer: London\\nQuestion: What is the capital of Australia?\\nAnswer: Canberra\\nQuestion: What is the capital of New Zealand?\\nAnswer: Wellington\\nQuestion: What is the capital of Canada?\\nAnswer: Ottawa\\nQuestion: What is the capital of Japan?']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = general_model(\n",
    "    \"\"\"Question: What is the capital of Country?\n",
    "Answer: City\n",
    "Question: What is the capital of France?\n",
    "Answer:\"\"\",\n",
    ")\n",
    "out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format of the answer is more predictable: if we needed to extract the answer for a downstream task it would be much easier with this prompt.\n",
    "Now the model still continues to generate questions and answer after it has answered.\n",
    "\n",
    "The pipeline supports a callback which lets us specify a string on which it terminate. We are going to stop on the string `Question:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1430a81c19f4a52ac491c0162a2bafc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-15 17:39:25 INFO: Attach to IPUs\n",
      "2023-02-15 17:39:25 INFO: Start inference\n",
      "Prompt: 'Question: What is the capital of Country?\n",
      "Answer: city\n",
      "Question: What is the capital of France?\n",
      "Answer:' Paris\n",
      "Question<|endoftext|>"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' Paris\\nQuestion<|endoftext|>']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = general_model(\n",
    "    f\"\"\"Question: What is the capital of Country?\n",
    "Answer: city\n",
    "Question: What is the capital of France?\n",
    "Answer:\"\"\",\n",
    "    terminate_on_string=\"Question:\",\n",
    ")\n",
    "out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our output is now the correct answer with some standard strings that can easily be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Paris']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capitals = [answer.splitlines()[0].strip() for answer in out]\n",
    "capitals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model configuration supports batched generation, with a batch of 16 this model can generate answers to 16 questions at a time. Lets generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0682f9db87f4706ac19fcd0f93bf298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-15 17:39:31 INFO: Attach to IPUs\n",
      "2023-02-15 17:39:31 INFO: Start inference\n",
      "Prompt: 'List countries in Europe, America, Asia and Africa: France, '\n",
      "Germany, \n",
      "Italy, \n",
      "Spain, \n",
      "United Kingdom, \n",
      "United States, \n",
      "Canada, \n",
      "Mexico, \n",
      "Argentina, \n",
      "Brazil, \n",
      "Chile, \n",
      "Colombia, \n",
      "Costa Rica, \n",
      "Ecuador, \n",
      "El Salvador, \n",
      "Guatemala, \n",
      "Honduras, \n",
      "Haiti, \n",
      "Jamaica, \n",
      "Mexico, \n",
      "Nicaragua, \n",
      "Panama, \n",
      "Paraguay, \n",
      "Peru, \n",
      "Puerto Rico, \n",
      "Uruguay"
     ]
    }
   ],
   "source": [
    "prompt_for_countries = \"List countries in Europe, America, Asia and Africa: France, \"\n",
    "out = general_model(prompt_for_countries, print_live=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['France',\n",
       " 'Germany',\n",
       " 'Italy',\n",
       " 'Spain',\n",
       " 'United Kingdom',\n",
       " 'United States',\n",
       " 'Canada',\n",
       " 'Mexico',\n",
       " 'Argentina',\n",
       " 'Brazil',\n",
       " 'Chile',\n",
       " 'Colombia',\n",
       " 'Costa Rica',\n",
       " 'Ecuador',\n",
       " 'El Salvador',\n",
       " 'Guatemala',\n",
       " 'Honduras',\n",
       " 'Haiti',\n",
       " 'Jamaica',\n",
       " 'Mexico',\n",
       " 'Nicaragua',\n",
       " 'Panama',\n",
       " 'Paraguay',\n",
       " 'Peru',\n",
       " 'Puerto Rico',\n",
       " 'Uruguay']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries = [\n",
    "    c.strip() for c in (prompt_for_countries + out[0]).split(\":\")[1].split(\",\")\n",
    "]\n",
    "countries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these generated countries, we compose a structured prompt for each and pass it off to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b350590397f34ef695aa7da1939a1651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-15 17:41:00 INFO: Attach to IPUs\n",
      "2023-02-15 17:41:00 INFO: Start inference\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('France', 'Paris'),\n",
       " ('Germany', 'Berlin'),\n",
       " ('Italy', 'Rome'),\n",
       " ('Spain', 'Madrid'),\n",
       " ('United Kingdom', 'London'),\n",
       " ('United States', 'Washington'),\n",
       " ('Canada', 'Ottawa'),\n",
       " ('Mexico', 'Mexico City'),\n",
       " ('Argentina', 'Buenos Aires'),\n",
       " ('Brazil', 'Brasilia'),\n",
       " ('Chile', 'Santiago'),\n",
       " ('Colombia', 'Bogota'),\n",
       " ('Costa Rica', 'San Jose'),\n",
       " ('Ecuador', 'Quito'),\n",
       " ('El Salvador', 'San Salvador'),\n",
       " ('Guatemala', 'Guatemala City'),\n",
       " ('Honduras', 'Tegucigalpa'),\n",
       " ('Haiti', 'Port-au-Prince'),\n",
       " ('Jamaica', 'Kingston'),\n",
       " ('Mexico', 'Mexico City'),\n",
       " ('Nicaragua', 'Managua'),\n",
       " ('Panama', 'Panama City'),\n",
       " ('Paraguay', 'AsunciÃ³n'),\n",
       " ('Peru', 'Lima'),\n",
       " ('Puerto Rico', 'San Juan'),\n",
       " ('Uruguay', 'Montevideo')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = general_model(\n",
    "    [\n",
    "        f\"\"\"Question: What is the capital of China?\n",
    "Answer: Beijing\n",
    "Question: What is the capital of {country}?\n",
    "Answer:\"\"\"\n",
    "        for country in countries\n",
    "    ],\n",
    "    print_live=False,\n",
    "    terminate_on_string=\"Question:\",\n",
    ")\n",
    "capitals = [answer.splitlines()[0].strip() for answer in out]\n",
    "list(zip(countries, capitals))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining entailment\n",
    "\n",
    "One common language task is to determine if statements agree, disagree or a neutral relative to each other.\n",
    "This task is commonly referred to as determining entailment and there are datasets like the [MNLI task of the GLUE dataset](https://huggingface.co/datasets/glue).\n",
    "\n",
    "The Mnli dataset consists of pairs of sentences, a *premise* and a *hypothesis*.\n",
    "The task is to predict the relation between the premise and the hypothesis, which can be:\n",
    "- `entailment`: hypothesis follows from the premise,\n",
    "- `contradiction`: hypothesis contradicts the premise,\n",
    "- `neutral`: hypothesis and premise are unrelated.\n",
    "\n",
    "We can use our generative model to tackle this task by creating prompts which have the following format:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hypothesis: The person is leaving. premise: Hello, welcome to the country. target: '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def entailment_prompt(hypothesis, premise, target=\"\"):\n",
    "    sep = \".\\n\" if target else \"\"\n",
    "    return f\"hypothesis: {hypothesis} premise: {premise} target: {target}{sep}\"\n",
    "\n",
    "\n",
    "entailment_prompt(\"The person is leaving.\", \"Hello, welcome to the country.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are attempting to get the model to correctly recognise that the statements:\n",
    "\n",
    " \"The person is leaving.\" and \"Hello, welcome.\" disagree with each other.\n",
    "\n",
    "This task is more complicated than the previous one so we provide more instructions and two examples to get the model to perform the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a97c61d065f4a13924491afca58eb28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-15 17:41:17 INFO: Attach to IPUs\n",
      "2023-02-15 17:41:17 INFO: Start inference\n",
      "Prompt: 'Tell me if the statements agree, disagree, neutral.\n",
      "Example - hypothesis: Goodbye. premise: Hey there. target: disagree.\n",
      "Example - hypothesis: Hello. premise: Hey there. target: agree.\n",
      "hypothesis: The person is leaving. premise: Hello, welcome. target: ' neutral.\n",
      "\n",
      "A:\n",
      "\n",
      "I would say that the first two are neutral, the third is a disagreement, and the fourth is a disagreement.\n",
      "The first two are neutral because they are"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' neutral.\\n\\nA:\\n\\nI would say that the first two are neutral, the third is a disagreement, and the fourth is a disagreement.\\nThe first two are neutral because they are']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entailment_instructions = (\n",
    "    \"Tell me if the statements agree, disagree, neutral.\\n\"\n",
    "    + \"Example - \"\n",
    "    + entailment_prompt(\"Goodbye.\", \"Hey there.\", \"disagree\")\n",
    "    + \"Example - \"\n",
    "    + entailment_prompt(\"Hello.\", \"Hey there.\", \"agree\")\n",
    ")\n",
    "general_model(\n",
    "    entailment_instructions\n",
    "    + entailment_prompt(\"The person is leaving.\", \"Hello, welcome.\"),\n",
    "    print_live=True,\n",
    "    output_length=40,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the model does pick one of the 3 options, it does not select the right one. If we let the answer continue running we see that the response feels relevant to the task but is quite imprecise.\n",
    "\n",
    "To do more thorough testing we can load the MNLI task from the GLUE dataset using the ðŸ¤— Datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-15 17:41:40 WARNING: Reusing dataset glue (/home/alexandrep/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'premise': 'Your contribution helped make it possible for us to provide our students with a quality education.',\n",
       " 'hypothesis': \"Your contributions were of no help with our students' education.\",\n",
       " 'label': 2,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "from data import mnli_data\n",
    "\n",
    "dataset = datasets.load_dataset(\"glue\", \"mnli\", split=\"validation_mismatched\")\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9092f6316aac478bb91eb6d8c10c219a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-15 17:41:43 INFO: Attach to IPUs\n",
      "2023-02-15 17:41:43 INFO: Start inference\n",
      "Prompt: 'Tell me if the statements agree, disagree, neutral.\n",
      "Example - hypothesis: Goodbye. premise: Hey there. target: disagree.\n",
      "Example - hypothesis: Hello. premise: Hey there. target: agree.\n",
      "hypothesis: Your contributions were of no help with our students' education. premise: Your contribution helped make it possible for us to provide our students with a quality education. target: ' neutral.\n",
      "\n",
      "A:\n",
      "\n",
      "I would"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('contradiction', 'neutral.'),\n",
       " ('contradiction', ''),\n",
       " ('entailment', 'We serve a classic Tuscan meal that includes a'),\n",
       " ('contradiction', 'neutral.'),\n",
       " ('entailment', 'neutral.'),\n",
       " ('entailment', 'neutral.'),\n",
       " ('contradiction', 'neutral.'),\n",
       " ('contradiction', ''),\n",
       " ('neutral', ''),\n",
       " ('contradiction', 'neutral.'),\n",
       " ('neutral', ''),\n",
       " ('contradiction', 'neutral.'),\n",
       " ('contradiction', 'neutral.'),\n",
       " ('contradiction', ''),\n",
       " ('contradiction', 'agree.'),\n",
       " ('neutral', 'neutral.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = general_model(\n",
    "    [\n",
    "        entailment_instructions + entailment_prompt(hypothesis, premise)\n",
    "        for hypothesis, premise in zip(\n",
    "            dataset[:16][\"hypothesis\"], dataset[:16][\"premise\"]\n",
    "        )\n",
    "    ],\n",
    "    print_live=True,\n",
    "    output_length=10,\n",
    ")\n",
    "# Strip out everything in the output after new lines\n",
    "mnli_id_2_class = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "[\n",
    "    (mnli_id_2_class[label], answer.splitlines()[0].strip())\n",
    "    for label, answer in zip(dataset[:16][\"label\"], out)\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see our prompts are not sufficient to use that model to complete this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_model.detach()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a fine-tuned model\n",
    "\n",
    "In order to complete the entailment task we are going to use a fine-tuned model on the MNLI task of the GLUE dataset.\n",
    "\n",
    "The checkpoint we will be using was fine-tuned on the Graphcore IPU and is hosted on the ðŸ¤— hub at [Graphcore/gptj-mnli](https://huggingface.co/Graphcore/gptj-mnli). As we did before we can load it in with a single command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-15 17:41:53 INFO: Creating session\n",
      "2023-02-15 17:41:53 INFO: Starting PopXL IR construction\n",
      "2023-02-15 17:42:14 INFO: PopXL IR construction duration: 0.34 mins\n",
      "2023-02-15 17:42:14 INFO: Starting PopXL compilation\n",
      "2023-02-15 17:42:16 INFO: PopXL compilation duration: 0.04 mins\n",
      "2023-02-15 17:42:16 INFO: Downloading 'Graphcore/gptj-mnli' pretrained weights and tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15T17:42:14.717167Z popart:popart 4054544.4054544 W: [Ir::setIsPrepared] setIsPrepared was already called. It should only be called once.\n",
      "WARNING: The compile time engine option debug.branchRecordTile is set to \"23551\" when creating the Engine. (At compile-tile it was set to 1471)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-15 17:43:07 INFO: Starting Loading HF pretrained model to IPU\n",
      "2023-02-15 17:44:19 INFO: Loading HF pretrained model to IPU duration: 1.19 mins\n"
     ]
    }
   ],
   "source": [
    "mnli_model = pipeline.IPUGPTJPipeline(\n",
    "    config,\n",
    "    \"Graphcore/gptj-mnli\",\n",
    "    sequence_length=256,\n",
    "    print_live=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the previous checkpoint, the model can handle arbitrary text generation questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5faa446bc34c4d2f82609af016ef4b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-15 17:44:25 INFO: Attach to IPUs\n",
      "2023-02-15 17:46:26 INFO: Start inference\n",
      "Prompt: 'Hey there', I'm a new member to the forum and I'm looking for some advice. I'm a college student and I'm looking to buy a new car. I'm not sure what kind of car I want to buy, but I'm leaning towards a Honda Accord. I'm looking for a car that's reliable, but I also want something that's fun to drive. I'm not sure if I want a V6 or a V8, but I'm leaning towards the V6. I'm also looking for a car that's fairly inexpensive. I'm not sure what kind of car I want to buy, but I'm leaning"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\", I'm a new member to the forum and I'm looking for some advice. I'm a college student and I'm looking to buy a new car. I'm not sure what kind of car I want to buy, but I'm leaning towards a Honda Accord. I'm looking for a car that's reliable, but I also want something that's fun to drive. I'm not sure if I want a V6 or a V8, but I'm leaning towards the V6. I'm also looking for a car that's fairly inexpensive. I'm not sure what kind of car I want to buy, but I'm leaning\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_model(\"Hey there\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However on the entailment task the performance should be much better, even without instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "accc546c6f89434c83382fbe77728bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-15 17:52:55 INFO: Attach to IPUs\n",
      "2023-02-15 17:52:55 INFO: Start inference\n",
      "Prompt: 'hypothesis: The person is leaving. premise: Hello, welcome. target: ' contradiction<|endoftext|>"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' contradiction<|endoftext|>']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_model(entailment_prompt(\"The person is leaving.\", \"Hello, welcome.\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It got it right! Those sentences are contradictory. Now let's try our samples from the GLUE dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b4886b30a0142d3b30b2e9589911eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-15 18:54:18 INFO: Attach to IPUs\n",
      "2023-02-15 18:54:18 INFO: Start inference\n",
      "Prompt: 'mnli hypothesis: Your contributions were of no help with our students' education. premise: Your contribution helped make it possible for us to provide our students with a quality education. target: ' contradiction<|endoftext|><|endoftext|>"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('contradiction', 'contradiction'),\n",
       " ('contradiction', 'neutral'),\n",
       " ('entailment', 'entailment'),\n",
       " ('contradiction', 'contradiction'),\n",
       " ('entailment', 'entailment'),\n",
       " ('entailment', 'neutral'),\n",
       " ('contradiction', 'contradiction'),\n",
       " ('contradiction', 'contradiction'),\n",
       " ('neutral', 'entailment'),\n",
       " ('contradiction', 'neutral'),\n",
       " ('neutral', 'neutral'),\n",
       " ('contradiction', 'neutral'),\n",
       " ('contradiction', 'neutral'),\n",
       " ('contradiction', 'contradiction'),\n",
       " ('contradiction', 'contradiction'),\n",
       " ('neutral', 'neutral')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = mnli_model(\n",
    "    [\n",
    "        entailment_prompt(hypothesis, premise)\n",
    "        for hypothesis, premise in zip(\n",
    "            dataset[:16][\"hypothesis\"], dataset[:16][\"premise\"]\n",
    "        )\n",
    "    ],\n",
    "    print_live=True,\n",
    "    output_length=10,\n",
    ")\n",
    "# Strip out everything in the output after new lines\n",
    "mnli_id_2_class = [\"entailment\", \"neutral\", \"contradiction\", \"unknown\"]\n",
    "[\n",
    "    (\n",
    "        mnli_id_2_class[label],\n",
    "        answer.splitlines()[0].strip().replace(\"<|endoftext|>\", \"\"),\n",
    "    )\n",
    "    for label, answer in zip(dataset[:16][\"label\"], out)\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gets almost all of them right, it clearly has some knowledge of the task we need it to complete.\n",
    "\n",
    "To take that one step further we can create a pipeline specific to this task which handles the prompt pre-processing and the post-processing of the generated text.\n",
    "\n",
    "In that way the generative model appears as a much simpler classifier. To avoid having to create a new IPU session with the same model, we can use the `from_gptj_pipeline` factory method, this gives us a ready to use pipeline for MNLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47cb2b55f60440da94e273ed6e67d805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ed2f4b218d43fa8cba4381ba18fecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-15 17:53:09 INFO: Attach to IPUs\n",
      "2023-02-15 17:53:09 INFO: Start inference\n",
      "Prompt: 'mnli hypothesis: Goodbye. premise: Hey there. target:' contradiction<|endoftext|>"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['contradiction']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_pipeline = pipeline.GPTJEntailmentPipeline.from_gptj_pipeline(mnli_model)\n",
    "mnli_pipeline(\"Hey there.\", \"Goodbye.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ba05678f75445399e1bd8ba374b212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8143792fdcd244bca1a910f271faff1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-15 19:02:06 INFO: Attach to IPUs\n",
      "2023-02-15 19:02:06 INFO: Start inference\n",
      "Prompt: 'mnli hypothesis: Your contributions were of no help with our students' education. premise: Your contribution helped make it possible for us to provide our students with a quality education. target:' contradiction<|endoftext|> entailment<|endoftext|> entailment<|endoftext|> contradiction<|endoftext|> entailment<|endoftext|> entailment<|endoftext|> entailment<|endoftext|> contradiction<|endoftext|> entailment<|endoftext|> entailment<|endoftext|> entailment<|endoftext|><|endoftext|>"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>entailment</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>entailment</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>True</td>\n",
       "      <td>entailment</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>True</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>True</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>False</td>\n",
       "      <td>neutral</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>True</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>True</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>False</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>True</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>True</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>True</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>True</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    correct          label     prediction\n",
       "0      True  contradiction  contradiction\n",
       "1      True  contradiction  contradiction\n",
       "2      True     entailment     entailment\n",
       "3      True  contradiction  contradiction\n",
       "4      True     entailment     entailment\n",
       "5      True     entailment     entailment\n",
       "6      True  contradiction  contradiction\n",
       "7      True  contradiction  contradiction\n",
       "8     False        neutral     entailment\n",
       "9      True  contradiction  contradiction\n",
       "10     True        neutral        neutral\n",
       "11    False  contradiction        neutral\n",
       "12     True  contradiction  contradiction\n",
       "13     True  contradiction  contradiction\n",
       "14     True  contradiction  contradiction\n",
       "15     True        neutral        neutral"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_size = 200\n",
    "out = mnli_pipeline(\n",
    "    premise=dataset[:sample_size][\"premise\"],\n",
    "    hypothesis=dataset[:sample_size][\"hypothesis\"],\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame(\n",
    "    [\n",
    "        (mnli_id_2_class[label] == answer, mnli_id_2_class[label], answer)\n",
    "        for label, answer in zip(dataset[:sample_size][\"label\"], out)\n",
    "    ],\n",
    "    columns=[\"correct\", \"label\", \"prediction\"],\n",
    ")\n",
    "results.head(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 160/200 correct\n"
     ]
    }
   ],
   "source": [
    "print(f\"Got {results['correct'].sum()}/{len(results)} correct\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the previous prompts we hand crafted in this notebook, the entailment pipeline uses the same prompt processing that was used during training of the MNLI model.\n",
    "By using the same prompt format now, as during training we maximise the performance and throughput of the model by limiting the size of the prompt to it's strict minimum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "mnli_pipeline.detach()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.1.0+1205_poptorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b98909bafd6dd6f1e7abe058b85e35058ed7070911f248b1c4db6b998dcd08ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
