{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc3c94cf",
   "metadata": {},
   "source": [
    "# GPT-J Finetuning\n",
    "\n",
    "Copyright (c) 2023 Graphcore Ltd."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924f34e1",
   "metadata": {},
   "source": [
    "[GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B) is a causal decoder-only transformer model which can be used for text-generation.\n",
    "Causal means that a causal mask is used in the decoder attention, so that each token has visibility on previous tokens only.\n",
    "\n",
    "Language models are very powerful because a huge variety of tasks can be formulated as a text-to-text problem and thus adapted to fit the generative setup, where the model is asked to correctly predict future tokens. This idea has been widely explored in [T5 paper: Exploring the Limits of Transfer Learning with a Unified\n",
    "Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf).\n",
    "\n",
    "In this example we apply this idea and finetune GPT-J as a Causal Language Model (CLM) for Text Entailment on [GLUE MNLI dataset](https://huggingface.co/datasets/glue#mnli).\n",
    "\n",
    "You can easily adapt this example to do your custom finetuning on several downstream tasks, such as Question Answering, Named Entity Recognition, Sentiment Analysis, Text Classification in general: you just need to prepare data in the right way.\n",
    "\n",
    "Our weights are also available as an HF checkpoint at [Graphcore/gptj-mnli]( https://huggingface.co/Graphcore/gptj-mnli)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bb8fe3",
   "metadata": {},
   "source": [
    "## Paperspace Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c91033",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef470286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "number_of_ipus = int(os.getenv(\"NUM_AVAILABLE_IPU\", 16))\n",
    "if number_of_ipus != 16:\n",
    "    raise ValueError(f\"This example need 16 IPUs to work. Detected {number_of_ipus}\")\n",
    "    \n",
    "os.environ[\"POPART_CACHE_DIR\"] = os.getenv(\"POPLAR_EXECUTABLE_CACHE_DIR\", \"cache\")\n",
    "checkpoint_dir =  os.getenv(\"CHECKPOINT_DIR\", \"checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b46154f",
   "metadata": {},
   "source": [
    "## Initial setup\n",
    "### Finetuning configuration\n",
    "First of all, we need to load the default configuration, defined in `config/finetuning_mnli.yml`.\n",
    "These are optimised configuration to run the model on IPUs.\n",
    "We need to pick the one suitable for a POD16.\n",
    "\n",
    "This configuration uses a sequence length of 1024 tokens. GPT-J layers are split across 16 IPUs, using [Tensor Model Parallelism](https://arxiv.org/pdf/1909.08053.pdf). No data parallelism is used (This extra optimization is available when using a POD64).\n",
    "\n",
    "The `gptj_fine_tuning_setup` setup the specified configuration, configures logging and Weight and Biases, and loads the Hugging Face pretrained model [EleutherAI/gpt-j-6B](https://huggingface.co/EleutherAI/gpt-j-6B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52deb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.setup import gptj_fine_tuning_setup\n",
    "from config import CONFIG_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bf40fb",
   "metadata": {},
   "source": [
    "> **W&B**: We support logging to Weights & Biases.\n",
    "If you want to use it, you will first need to manually log in (see the quickstart guide [here](https://docs.wandb.ai/quickstart)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd02f4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to True if you want to use W&B. Be sure to be logged in.\n",
    "wandb_setup = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4111bd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a configuration\n",
    "config, args, pretrained = gptj_fine_tuning_setup(\n",
    "    CONFIG_DIR / \"finetuning_mnli.yml\", \"release\", \"gptj_6B_1024_pod16\", wandb_setup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1c4bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config.dumps_yaml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50640a65",
   "metadata": {},
   "source": [
    "### Validation Configuration\n",
    "A default configuration for inference-only is available in `config/inference.yml`.\n",
    "There are two such configurations:\n",
    "- one is `gpt-j`. This is the base configuration, and is guaranteed to fit into memory with 1024 sequence length.\n",
    "- `gpt-j-mnli` is slightly optimised for the mnli dataset. Indeed, the sequence length can be reduced and more batches can fit into memory.\n",
    "\n",
    "In this example we will start from the default one, and manually reduce the sequence length and increase the batch size later on.\n",
    "You can do the same on your custom dataset to find the best configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584a190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.setup import gptj_config_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598aaa10",
   "metadata": {},
   "source": [
    "> **W&B** We support logging to Weights & Biases.\n",
    "If you want to use it, you will first need to manually log in (see the quickstart guide [here](https://docs.wandb.ai/quickstart))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7644616a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_setup_on_eval = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369850fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_config, args, _ = gptj_config_setup(\n",
    "    CONFIG_DIR / \"inference.yml\", \"release\", \"gpt-j\", hf_model_setup=False, wandb_setup=wandb_setup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6091ef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_config.dumps_yaml())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d42427a7",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "MNLI dataset consists of pairs of sentences, a *premise* and a *hypothesis*.\n",
    "The task is to predict the relation between the premise and the hypothesis, which can be:\n",
    "- `entailment`: hypothesis follows from the premise,\n",
    "- `contradiction`: hypothesis contradicts the premise,\n",
    "- `neutral`: hypothesis and premise are unrelated.\n",
    "\n",
    "Data splits for the mnli dataset are the following:\n",
    "\n",
    "|train |validation_matched|validation_mismatched|\n",
    "|-----:|-----------------:|--------------------:|\n",
    "|392702|              9815|                 9832|\n",
    "\n",
    "\n",
    "You can explore it [on hugginface](https://huggingface.co/datasets/glue/viewer/mnli/train).\n",
    "![MNLI dataset](imgs/mnli_dataset.png)\n",
    "\n",
    "\n",
    "### Traning Preprocessing\n",
    "The columns we are interested in are `hypothesis`, `premise` and `label`.\n",
    "\n",
    "The first step consists in forming input prompts with the format\n",
    "```bash\n",
    "mnli hypothesis: {hypothesis} premise: {premise} target: {label} <|endoftext|>\n",
    "```\n",
    "For example:\n",
    "```\n",
    "mnli hypothesis: Product and geography are what make cream skimming work.  premise: Conceptually cream skimming has two basic dimensions - product and geography. target: neutral<|endoftext|>\n",
    "```\n",
    "\n",
    "Then, prompt sentences are tokenized and packed together to form 1024 token sequences, following [HF packing algorithm](https://github.com/huggingface/transformers/blob/v4.20.1/examples/pytorch/language-modeling/run_clm.py). No padding is used.\n",
    "\n",
    "Finally, the prompt is split into `input_ids` and `labels`. The input consists of the full sentence but for the last token (`prompt[:-1]`), and the label is the sentence shifted by one (`prompt[1:]`).\n",
    "Given the training format, no extra care is needed to account for different sequences: the model does not need to know which sentence a token belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad25a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import data.hf_data_utils as hf_data_utils\n",
    "import data.mnli_data as mnli_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3bc59b",
   "metadata": {},
   "source": [
    "The next two cells are the ones you want to change in a custom finetuning.\n",
    "\n",
    "We first load the MNLI dataset, and then create a custom preprocessing function to build prompts suitable for a\n",
    "text-to-text setup.\n",
    "In a custom finetuning, you will need to choose a format for your prompts and change the `form_training_prompts` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd5fe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HF dataset\n",
    "dataset = load_dataset(\"glue\", \"mnli\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81417cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dd8fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form prompts in the format mnli hypothesis: {hypothesis} premise: {premise} target: {class_label} <|endoftext|>\n",
    "def form_training_prompts(example):\n",
    "    hypothesis = example[\"hypothesis\"]\n",
    "    premise = example[\"premise\"]\n",
    "    class_label = [\"entailment\", \"neutral\", \"contradiction\"][example[\"label\"]]\n",
    "\n",
    "    example[\"text\"] = f\"mnli hypothesis: {hypothesis} premise: {premise} target: {class_label}<|endoftext|>\"\n",
    "    return example\n",
    "\n",
    "form_training_prompts(dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608877e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "    form_training_prompts,\n",
    "    remove_columns=[\"hypothesis\", \"premise\", \"label\", \"idx\"],\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Generating text prompt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74013745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows first textual prompt\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98315389",
   "metadata": {},
   "source": [
    "After that, we tokenize the prompts. You won't need to change this step in a custom finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7ad3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<|extratoken_1|>\"})  # index 50257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd82124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize prompts\n",
    "dataset = dataset.map(\n",
    "    mnli_data.tokenizes_text(tokenizer),\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=1,\n",
    "    remove_columns=dataset.column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Tokenizing text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb61703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows first tokenized prompt\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30698106",
   "metadata": {},
   "source": [
    "Finally, we use the HF packing algorithm (`group_text`) to create packed sentences of the specified sequence length,\n",
    "and separate inputs and labels.\n",
    "Again, this is a step you are not going to change in a custom finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b402f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pack tokenized prompts into sequences and split sequences in input_ids and labels\n",
    "dataset = dataset.map(\n",
    "    hf_data_utils.group_texts(config),\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=1,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Packing sequences\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8916c34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5757285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a portion of first sentence. You can see that the label is the input shifted by one.\n",
    "print(\"first 10 tokens of first sentence\")\n",
    "print(\"input_ids\")\n",
    "print(dataset[\"input_ids\"][0][:10])\n",
    "print(\"labels - shifted by one\")\n",
    "print(dataset[\"labels\"][0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac769c10",
   "metadata": {},
   "source": [
    "> **Note** If you want to adapt this code to another dataset, be sure to call the inputs and labels in the same way: `input_ids` and `labels`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee766c7",
   "metadata": {},
   "source": [
    "### Validation Preprocessing\n",
    "For validation, we use the [MNLI validation_mismatched](https://huggingface.co/datasets/glue/viewer/mnli_mismatched/validation) split.\n",
    "\n",
    "\"Mismatched\" means validation examples are not derived from the same sources as those in the training set and therefore not closely resembling any of the examples seen at training time.\n",
    "\n",
    "Similarly to what we did for training, the first preprocessing step is creating prompts, this time without including the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8027a00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_validation_prompts(example):\n",
    "    hypothesis = example[\"hypothesis\"]\n",
    "    premise = example[\"premise\"]\n",
    "    class_label = [\"entailment\", \"neutral\", \"contradiction\"][example[\"label\"]]\n",
    "\n",
    "    example[\"text\"] = f\"mnli hypothesis: {hypothesis} premise: {premise} target:\"\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e416a200",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = load_dataset(\"glue\", \"mnli\", split=\"validation_mismatched\")\n",
    "eval_dataset = eval_dataset.map(form_validation_prompts,\n",
    "                                remove_columns=[\"hypothesis\", \"premise\", \"idx\"],\n",
    "                                load_from_cache_file=False,\n",
    "                                desc=\"Generating text prompt\",\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b53294",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb78148",
   "metadata": {},
   "source": [
    "Finally, input prompts are tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb50d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_validation_features(dataset, tokenizer):\n",
    "    tokenized_examples = []\n",
    "    for example in dataset[\"text\"]:\n",
    "        tokenized_example = tokenizer.encode(example, return_tensors=\"pt\").squeeze()\n",
    "        tokenized_examples.append(tokenized_example)\n",
    "    return {\"input_ids\": tokenized_examples, \"label\": dataset[\"label\"]}\n",
    "\n",
    "eval_dataset = eval_dataset.map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names,\n",
    "    load_from_cache_file=False,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df352f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4bda49",
   "metadata": {},
   "source": [
    "> **Note** If you want to adapt this code to another dataset, be sure to call the inputs and labels in the same way: `input_ids` and `label`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ffffd5",
   "metadata": {},
   "source": [
    "## Customise configuration and create a Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d787491b",
   "metadata": {},
   "source": [
    "Right at the beginning of the notebook we have loaded some default configurations for training and inference. We are now going to show how to customise some of the parameters for your needs.\n",
    "\n",
    "### Customise training configuration\n",
    "In the cells below we list the parameters you are most likely to play around when doing a custom finetuning.\n",
    "\n",
    "These are the training steps, dropout probability and optimizer/learning rate parameters.\n",
    "\n",
    "Moreover, it is important that you specify **checkpoints** parameters, namely a folder to save the finetuned weights and a periodicity for checkpointing. Be aware that saving checkpoints takes time, so you don't want to save them too often.\n",
    "To disable intermediate checkpoints set `config.checkpoint.steps = 0`. The final checkpoint is always saved provided the save directory is given. Set it to `None` if you don't want to save weights, but it's unlikely you want to disable the last checkpoint.\n",
    "\n",
    "If you are not resuming training, and you don't care about resuming the training later on you can reduce the time and memory required to save checkpoints by specifying `optim_state=False`. In this case, only the model weights will be saved, while the optimiser state will be discarded.\n",
    "\n",
    "Checkpoints will be saved in the directory given by the environment variable `CHECKPOINT_DIR`, which we saved in `checkpoint_dir` at the beginning. You can have a look to the path printing it out in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04dcb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46498d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customise training arguments\n",
    "config.model.dropout_prob = 0.0 \n",
    "config.training.steps = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ba5f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customise optimiser and learning rate schedule\n",
    "config.training.optimizer.learning_rate.maximum = 5e-06\n",
    "config.training.optimizer.learning_rate.warmup_proportion = 0.005995\n",
    "config.training.optimizer.learning_rate.beta1 = 0.9\n",
    "config.training.optimizer.learning_rate.beta2 = 0.999\n",
    "config.training.optimizer.learning_rate.weight_decay = 0.0\n",
    "config.training.optimizer.learning_rate.gradient_clipping = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c25598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customise checkpoints\n",
    "config.checkpoint.save = checkpoint_dir # where the model is saved. None means don't save any checkpoint.\n",
    "config.checkpoint.steps = 100  # how often you save the model. 0 means only the final checkpoint is saved.\n",
    "config.checkpoint.to_keep = 4  # maximum number of checkpoints kept on disk\n",
    "config.checkpoint.optim_state = False # Whether to include the optimiser state in checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef98315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume training\n",
    "config.checkpoint.load = None # you can specify a directory containing a previous checkpoint,\n",
    "                              # os.path.join(checkpoint_dir, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bb5a0a",
   "metadata": {},
   "source": [
    "### Customise validation configuration\n",
    "For validation there are less relevant parameters.\n",
    "\n",
    "You can control the maximum number of tokens generated by the model using the  `output_length` parameter. If you know that the targets are only few tokens long, it is convenient to set it to a small number. Generation stops if the output token is `<|endoftext|>` or after `output_length` tokens are generated.\n",
    "\n",
    "In our case, we set the output_len to 5 to accommodate all class labels and the `<|endoftext|>` token.\n",
    "\n",
    "You can also reduce the model `sequence_len` to account for the maximum length of sentences encountered in the validation dataset, plus the `output_length` specified.\n",
    "In our case of MNLI example, `sequence_len = 229` and this allows us to increase the batch size to `16`.\n",
    "\n",
    "If you adapt this example to another dataset, you can compare your dataset `max_len` with ours and decide if you can safetly use the increased batch size. Otherwise, stick with the default one or create your own.\n",
    "\n",
    "You can specify a checkpoint to be used for validation. If `None` is provided, the latest weights will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ec3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730fd2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify maximum number of tokens that can be generated.\n",
    "eval_config.inference.output_length = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41066678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing sequence length\n",
    "max_len = reduce(lambda l, e: max(l, len(e[\"input_ids\"])), eval_dataset, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246a929f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Maximum length in mnli-mismatched dataset: {max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89adbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_config.model.sequence_length = max_len + eval_config.inference.output_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d7a1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Setting sequence length to {eval_config.model.sequence_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d59fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can specify a directory containing a previous checkpoint.\n",
    "# None means latest weights are used\n",
    "config.checkpoint.load = None # os.path.join(checkpoint_dir, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2be9644",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config.dumps_yaml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e494016",
   "metadata": {},
   "source": [
    "Finally, we need to define a validation metric. We will use `accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cc9b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed62517",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d9aaf6",
   "metadata": {},
   "source": [
    "We also need to define a function to convert our generated labels to integer indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12b6b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_mnli_predictions(generated_sentences):\n",
    "    labels_to_ids = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2, \"unknown\": -1}\n",
    "    predictions = []\n",
    "    for s in generated_sentences:\n",
    "        answer = mnli_data.extract_class_label(s)\n",
    "        predictions.append(labels_to_ids[answer])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de376a5",
   "metadata": {},
   "source": [
    "### Create a Trainer\n",
    "Once a config is specified, we are ready to create the training session with the help of the \n",
    "`MNLIFinetuningTrainer` class.\n",
    "You need to provide the following arguments:\n",
    "\n",
    "- *config*: the training configuration.\n",
    "- *pretrained*: the Hugging Face pre-trained model, used to initialise the weights.\n",
    "- *dataset*: the training dataset.\n",
    "\n",
    "Moreover, you can specify:\n",
    "\n",
    "- *eval_dataset*: the validation dataset.\n",
    "- *eval_config*: the inference configuration, to be used in validation.\n",
    "- *tokenizer*: the tokenizer, needed by validation.\n",
    "- *metric*: the metric for validation. An Hugging Face metric from `evaluate` module, or a custom function. We use the `accuracy` metric.\n",
    "- *process_answers_func*: a function to convert the generated answers to the format required by the metric and the labels. For example we need to convert textual categories `[entailment, contradiction,neutral]` to indices.\n",
    "\n",
    "These extra arguments can also be provided later on when calling `trainer.evaluate(...)`.\n",
    "\n",
    "The first time you run this notebook, it will take around 10 minutes to compile the training model and about 4 minutes to compile the inference model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c54b63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.trainer import MNLIFinetuningTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4814f303",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MNLIFinetuningTrainer(config,\n",
    "                                pretrained,\n",
    "                                dataset,\n",
    "                                eval_dataset,\n",
    "                                eval_config,\n",
    "                                tokenizer,\n",
    "                                clf_metrics,\n",
    "                                postprocess_mnli_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e9ae9a",
   "metadata": {},
   "source": [
    "## Run Finetuning\n",
    "We can now run training for the number of steps you set in the config.\n",
    "Checkpoints will be saved in the folder you specified in `config.save`, with the periodicity identified by `config.checkpoint.steps`.\n",
    "\n",
    "Training takes around 8 minutes to start because the pretrained weights need to be loaded to the IPUs.\n",
    "After that, each step takes around 22 seconds, processing 5.7 samples/s.\n",
    "\n",
    "Each sample consists of 1024 tokens, so the performance is \n",
    "\n",
    "$$\n",
    "\\text{Throughput} = 1024*5.7 \\approx 5800 \\, \\text{tokens/s}\n",
    "$$\n",
    "\n",
    "You can estimate how much it would take to do finetuning on your dataset by plugging this velocity\n",
    "\n",
    "$$\n",
    "\\text{time} = \\frac{\\text{dataset_size(tokens)}}{\\text{tokens/s}} * \\text{epochs}\n",
    "$$\n",
    "\n",
    "This time does not include checkpoint time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6013c20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed7ce99",
   "metadata": {},
   "source": [
    "## Run validation\n",
    "Finally, we validate our model on [mnli-mismatched](https://huggingface.co/datasets/glue/viewer/mnli_mismatched/test) split of MNLI dataset.\n",
    "\n",
    "Generative inference is performed token-by-token using a greedy heuristic: the next token is chosen based on the highest logits.\n",
    "\n",
    "We run token-by-token inference in batches to generate labels for multiple examples at once.\n",
    "We then compute the accurancy comparing the model answers with the true labels.\n",
    "\n",
    "The resulting model matches SOTA performance with 82.5% accuracy.\n",
    "\n",
    "```\n",
    "Total number of examples                 9832\n",
    "Number with badly formed result          0\n",
    "Number with incorrect result             1725\n",
    "Number with correct result               8107 \n",
    "[82.5%]\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb57760",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bfb307",
   "metadata": {},
   "source": [
    "## Save HF checkpoint\n",
    "You can save the trained weights so that they can be uploaded to Hugging Face and used in Hugging Face torch model.\n",
    "You can specify a checkpoint path if you want to convert a specific checkpoint, instead of the latest weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0335492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_checkpoint_path = \"hf_checkpoint\"\n",
    "ckpt_path = None # os.path.join(checkpoint_dir, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac567deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned = trainer.save_hf_checkpoint(hf_checkpoint_path, ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3e7bf2",
   "metadata": {},
   "source": [
    "The same model can be later used with standard HF pipeline on any hardware.\n",
    "\n",
    "```python\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-j-6B')\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\"Graphcore/gptj-mnli\", pad_token_id=tokenizer.eos_token_id)\n",
    "generator =  pipeline('text-generation', model=hf_model, tokenizer=tokenizer)\n",
    "\n",
    "prompt = \"mnli hypothesis: Your contributions were of no help with our students' education.\" \\\n",
    "         \"premise: Your contribution helped make it possible for us to provide our students with a quality education. target:\"\n",
    "\n",
    "out = generator(prompt, return_full_text=False, max_new_tokens=5, top_k=1)\n",
    "# [{'generated_text': ' contradiction'}]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
