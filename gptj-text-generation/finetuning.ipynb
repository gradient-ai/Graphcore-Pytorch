{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc3c94cf",
   "metadata": {},
   "source": [
    "# GPT-J Finetuning\n",
    "\n",
    "Copyright (c) 2023 Graphcore Ltd."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924f34e1",
   "metadata": {},
   "source": [
    "[GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B) is a causal decoder-only transformer model which can be used for text-generation.\n",
    "Causal means that a causal mask is used in the decoder attention, so that each token has visibility on previous tokens only.\n",
    "\n",
    "Language models are very powerful because a huge variety of tasks can be formulated as a text-to-text problem and thus adapted to fit the generative setup, where the model is asked to correctly predict future tokens. This idea has been widely explored in [T5 paper: Exploring the Limits of Transfer Learning with a Unified\n",
    "Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf)\n",
    "\n",
    "In this example we apply this idea and finetune GPT-J as a Causal Language Model (CLM) for Text Entailment on [GLUE MNLI dataset](https://huggingface.co/datasets/glue#mnli).\n",
    "\n",
    "You can easily adapt this example to do your custom finetuning on several downstream tasks, such as Question Answering, Named Entity Recognition, Sentiment Analysis, Text Classification: you just need to prepare data in the right way.\n",
    "\n",
    "Our weights are also available as an HF checkpoint at [Graphcore/gptj-mnli]( https://huggingface.co/Graphcore/gptj-mnli)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bb8fe3",
   "metadata": {},
   "source": [
    "## Paperspace Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c91033",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef470286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "number_of_ipus = int(os.getenv(\"NUM_AVAILABLE_IPU\", 16))\n",
    "if number_of_ipus != 16:\n",
    "    raise ValueError(f\"This example need 16 IPUs to work. Detected {number_of_ipus}\")\n",
    "    \n",
    "os.environ[\"POPART_CACHE_DIR\"] = os.getenv(\"POPLAR_EXECUTABLE_CACHE_DIR\", \"cache\")\n",
    "checkpoint_dir =  os.getenv(\"CHECKPOINT_DIR\", \"checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b46154f",
   "metadata": {},
   "source": [
    "## Finetuning\n",
    "\n",
    "### Configuration\n",
    "First of all, we need to load the default configuration, defined in `config/finetuning_mnli.yml`.\n",
    "These are optimised configuration to run the model on IPUs.\n",
    "We need to pick the one suitable for a POD16.\n",
    "\n",
    "This configuration uses a sequence length of 1024 tokens. GPT-J layers are split across 16 IPUs, using [Tensor Model Parallelism](https://arxiv.org/pdf/1909.08053.pdf). No data parallelism is used (This extra optimization is available when using a POD64).\n",
    "\n",
    "The `gptj_fine_tuning_setup` setup the specified configuration, configures logging and Weight and Biases, and loads the Hugging Face pretrained model [EleutherAI/gpt-j-6B](https://huggingface.co/EleutherAI/gpt-j-6B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52deb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.setup import gptj_fine_tuning_setup\n",
    "from config import CONFIG_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bf40fb",
   "metadata": {},
   "source": [
    "> **W&B**: We support logging to Weights & Biases.\n",
    "If you want to use it, you will first need to manually log in (see the quickstart guide [here](https://docs.wandb.ai/quickstart)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd02f4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to True if you want to use W&B. Be sure to be logged in.\n",
    "wandb_setup = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4111bd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a configuration\n",
    "config, args, pretrained = gptj_fine_tuning_setup(\n",
    "    CONFIG_DIR / \"finetuning_mnli.yml\", \"release\", \"gptj_6B_1024_pod16\", wandb_setup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1c4bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config.dumps_yaml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42427a7",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Mnli dataset consists of pairs of sentences, a *premise* and a *hypothesis*.\n",
    "The task is to predict the relation between the premise and the hypothesis, which can be:\n",
    "- `entailment`: hypothesis follows from the premise,\n",
    "- `contradiction`: hypothesis contradicts the premise,\n",
    "- `neutral`: hypothesis and premise are unrelated.\n",
    "\n",
    "You can explore the [MNLI dataset on hugginface](https://huggingface.co/datasets/glue/viewer/mnli/train).\n",
    "![MNLI dataset](imgs/mnli_dataset.png)\n",
    "\n",
    "#### Preprocessing\n",
    "The columns we are interested in are `hypothesis`, `premise` and `label`.\n",
    "\n",
    "The first step consists in forming input prompts with the format\n",
    "```bash\n",
    "mnli hypothesis: {hypothesis} premise: {premise} target: {class_label} <|endoftext|>\n",
    "```\n",
    "For example:\n",
    "```\n",
    "mnli hypothesis: Your contributions were of no help with our students' education. premise: Your contribution helped make it possible for us to provide our students with a quality education. target: contradiction <|endoftext|>\n",
    "```\n",
    "\n",
    "Then, prompt sentences are tokenized and packed together to form 1024 token sequences, following [HF packing algorithm](https://github.com/huggingface/transformers/blob/v4.20.1/examples/pytorch/language-modeling/run_clm.py). No padding is used.\n",
    "\n",
    "Finally, the prompt is split into `input_ids` and `labels`. The input consists of the full sentence but for the last token (`prompt[:-1]`), and the label is the sentence shifted by one (`prompt[1:]`).\n",
    "Given the training format, no extra care is needed to account for different sequences: the model does not need to know which sentence a token belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad25a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import data.hf_data_utils as hf_data_utils\n",
    "import data.mnli_data as mnli_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3bc59b",
   "metadata": {},
   "source": [
    "The next two cells are the ones you want to change in a custom finetuning.\n",
    "\n",
    "We first load the MNLI dataset, and then create a custom preprocessing function to build prompts suitable for a\n",
    "text-to-text setup.\n",
    "In a custom finetuning, you will need to choose a format for your prompts and change the `form_text` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd5fe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HF dataset\n",
    "dataset = load_dataset(\"glue\", \"mnli\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81417cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dd8fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form prompts in the format mnli hypothesis: {hypothesis} premise: {premise} target: {class_label} <|endoftext|>\n",
    "def form_text(example):\n",
    "    hypothesis = example[\"hypothesis\"]\n",
    "    premise = example[\"premise\"]\n",
    "    class_label = [\"entailment\", \"neutral\", \"contradiction\"][example[\"label\"]]\n",
    "\n",
    "    example[\"text\"] = f\"mnli hypothesis: {hypothesis} premise: {premise} target: {class_label}<|endoftext|>\"\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608877e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "    form_text,\n",
    "    remove_columns=[\"hypothesis\", \"premise\", \"label\", \"idx\"],\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Generating text prompt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74013745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows first textual prompt\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98315389",
   "metadata": {},
   "source": [
    "After that, we tokenize the prompts. You won't need to change this step in a custom finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd82124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize prompts\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<|extratoken_1|>\"})  # index 50257\n",
    "dataset = dataset.map(\n",
    "    mnli_data.tokenizes_text(tokenizer),\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=1,\n",
    "    remove_columns=dataset.column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Tokenizing text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb61703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows first tokenized prompt\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30698106",
   "metadata": {},
   "source": [
    "Finally, we use the HF packing algorithm (`group_text`) to create packed sentences of the specified sequence length,\n",
    "and separate inputs and labels.\n",
    "Again, this is a step you are not going to change in a custom finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b402f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pack tokenized prompts into sequences and split sequences in input_ids and labels\n",
    "dataset = dataset.map(\n",
    "    hf_data_utils.group_texts(config),\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=1,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Packing sequences\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8916c34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5757285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a portion of first sentence. You can see that the label is the input shifted by one.\n",
    "print(\"first 10 tokens of first sentence\")\n",
    "print(\"input_ids\")\n",
    "print(dataset[\"input_ids\"][0][:10])\n",
    "print(\"labels - shifted by one\")\n",
    "print(dataset[\"labels\"][0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ffffd5",
   "metadata": {},
   "source": [
    "### Customise configuration and create a Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d787491b",
   "metadata": {},
   "source": [
    "In the cells below we list the parameters you are most likely to play around when doing a custom finetuning.\n",
    "\n",
    "These are the training steps, dropout probability and optimizer/learning rate parameters.\n",
    "\n",
    "Moreover, it is important that you specify **checkpoints** parameters, namely a folder to save the finetuned weights and a periodicity for checkpointing. Be aware that saving checkpoints takes time, so you don't want to save them too often.\n",
    "To disable intermediate checkpoints set `config.checkpoint.steps = 0`. The final checkpoint is always saved provided the save directory is given. Set it to `None` if you don't want to save weights.\n",
    "\n",
    "If you are not resuming training, and you don't care about resuming the training later on but you still want to save the model weights at different training steps, you can reduce the time and memory required to save checkpoints by specifying `optim_state=False` when creating the session.\n",
    "\n",
    "Checkpoints will be saved in the directory given by the environment variable `CHECKPOINT_DIR`, which we saved in `checkpoint_dir` at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04dcb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46498d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customise training arguments\n",
    "config.model.dropout_prob = 0.0 \n",
    "config.training.steps = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ba5f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customise optimiser and learning rate schedule\n",
    "config.training.optimizer.learning_rate.maximum = 5e-06\n",
    "config.training.optimizer.learning_rate.warmup_proportion = 0.005995\n",
    "config.training.optimizer.learning_rate.beta1 = 0.9\n",
    "config.training.optimizer.learning_rate.beta2 = 0.999\n",
    "config.training.optimizer.learning_rate.weight_decay = 0.0\n",
    "config.training.optimizer.learning_rate.gradient_clipping = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c25598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customise checkpoints\n",
    "config.checkpoint.save = checkpoint_dir # where the model is saved. None means don't save any checkpoint.\n",
    "config.checkpoint.steps = 100  # how often you save the model. 0 means only the final checkpoint is saved.\n",
    "config.checkpoint.to_keep = 4  # maximum number of checkpoints kept on disk\n",
    "config.checkpoint.optim_state = False # Whether to include the optimiser state in checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef98315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume training\n",
    "config.checkpoint.load = None # you can specify a directory containing a previous checkpoint,\n",
    "                              # os.path.join(checkpoint_dir, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2be9644",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config.dumps_yaml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de376a5",
   "metadata": {},
   "source": [
    "Once a config is specified, we are ready to create the training session with the help of the \n",
    "`MNLIFinetuningTrainer` class.\n",
    "You need to provide the following arguments:\n",
    "\n",
    "- *config*: the training configuration\n",
    "- *pretrained*: the Hugging Face pre-trained model, used to initialise the weights\n",
    "- *dataset*: the training dataset.\n",
    "\n",
    "Moreover, you can specify:\n",
    "\n",
    "- *eval_dataset*: the validation dataset\n",
    "- *eval_config*: the inference configuration, to be used in validation\n",
    "- *tokenizer*: the tokenizer, needed by validation\n",
    "\n",
    "These extra arguments can also be provided later on when calling `trainer.evaluate(...)`.\n",
    "\n",
    "The first time you run this notebook, it will take around 10 minutes to compile the training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c54b63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.trainer import MNLIFinetuningTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4814f303",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MNLIFinetuningTrainer(config, pretrained, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e9ae9a",
   "metadata": {},
   "source": [
    "### Run Finetuning\n",
    "We are done! We can now run training for the number of steps you set in the config.\n",
    "Checkpoints will be saved in the folder you specified in `config.save`, with the periodicity identified by `config.checkpoint.steps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6013c20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50640a65",
   "metadata": {},
   "source": [
    "## Validation\n",
    "We can now validate our model on [mnli-mismatched](https://huggingface.co/datasets/glue/viewer/mnli_mismatched/test) split of MNLI dataset.\n",
    "\n",
    "Generative inference is performed token-by-token using a greedy heuristic: the next token is chosen based on the highest logits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cfa2d9",
   "metadata": {},
   "source": [
    "### Config\n",
    "A default configuration for inference-only is available in `config/inference.yml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584a190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.setup import gptj_config_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598aaa10",
   "metadata": {},
   "source": [
    "> **W&B** We support logging to Weights & Biases.\n",
    "If you want to use it, you will first need to manually log in (see the quickstart guide [here](https://docs.wandb.ai/quickstart))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7644616a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_setup = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369850fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_config, args, _ = gptj_config_setup(\n",
    "    CONFIG_DIR / \"inference.yml\", \"release\", \"gpt-j-mnli\", hf_model_setup=False, wandb_setup=wandb_setup\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08d639b",
   "metadata": {},
   "source": [
    "The only interesting parameter for inference is `output_length`. This is the maximum number of tokens you want the model to generate during validation. If you know that the targets are only a few tokens long, it is convenient to set it to a small number.\n",
    "In our case, we set the output_len to 5 to accommodate all class labels and the `<|endoftext|>` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a658e084",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_config.inference.output_length = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee766c7",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "First of all we need to prepare the prompts, similarly to what we did for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e416a200",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = load_dataset(\"glue\", \"mnli\", split=\"validation_mismatched\")\n",
    "eval_dataset = eval_dataset.map(form_text,\n",
    "                                remove_columns=[\"hypothesis\", \"premise\", \"label\", \"idx\"],\n",
    "                                load_from_cache_file=False,\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b53294",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8072aa11",
   "metadata": {},
   "source": [
    "Now we want separate the input prompts, to be fed to the model, from the labels, which we need later to compute the accurancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e157b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = eval_dataset.map(mnli_data.split_text, load_from_cache_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3374b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb78148",
   "metadata": {},
   "source": [
    "Finally, input prompts are tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb50d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_validation_features(dataset, tokenizer):\n",
    "    tokenized_examples = []\n",
    "    for example in dataset[\"prompt_text\"]:\n",
    "        tokenized_example = tokenizer.encode(example, return_tensors=\"pt\").squeeze()\n",
    "        tokenized_examples.append(tokenized_example)\n",
    "    return {\"input_ids\": tokenized_examples, \"class_label\": dataset[\"class_label\"]}\n",
    "\n",
    "eval_dataset = eval_dataset.map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names,\n",
    "    load_from_cache_file=False,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df352f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed7ce99",
   "metadata": {},
   "source": [
    "### Run validation\n",
    "Now that we have preprocessed the dataset, we can compute the maximum length of sequences, `max_len`, and use this value to define the model `sequence_len`.\n",
    "\n",
    "Each sequence is right-padded to `max_len + output_len`. We use right padding so that padded tokens are never attended, thanks to the causal mask.\n",
    "\n",
    "GPTJTokenizer has no native padding token. However, we can safetly use the first `<|extratoken_1|>`.\n",
    "\n",
    "Padded sequences are fed to the model and generative inference is performed token-by-token: each time a new token is generated, it replaces a padding token, and the new sequence is fed back to the model.\n",
    "\n",
    "To increase efficiency, we perform inference on micro batches.\n",
    "\n",
    "Finally, we retrieve literal labels detokenizing the predictions and we compute the accuracy comparing the result with the expected one.\n",
    "\n",
    "If you want to evaluate a specific checkpoint, you can provide a `ckpt_load_path`. Otherwise, the latest weights will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84005414",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_load_path = None # os.path.join(checkpoint_dir, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb57760",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(eval_dataset,eval_config,tokenizer, ckpt_load_path=ckpt_load_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bfb307",
   "metadata": {},
   "source": [
    "## Save HF checkpoint\n",
    "You can save the trained weights so that they can be uploaded to Hugging Face and used in Hugging Face torch model.\n",
    "You can specify a checkpoint path if you want to convert a specific checkpoint, instead of the latest weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0335492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_checkpoint_path = \"hf_checkpoint\"\n",
    "ckpt_path = None # os.path.join(checkpoint_dir, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac567deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned = trainer.save_hf_checkpoint(hf_checkpoint_path, ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3e7bf2",
   "metadata": {},
   "source": [
    "The same model can be later used with standard HF pipeline on any hardware.\n",
    "\n",
    "```python\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-j-6B')\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\"Graphcore/gptj-mnli\", pad_token_id=tokenizer.eos_token_id)\n",
    "generator =  pipeline('text-generation', model=hf_model, tokenizer=tokenizer)\n",
    "\n",
    "prompt = \"mnli hypothesis: Your contributions were of no help with our students' education.\" \\\n",
    "         \"premise: Your contribution helped make it possible for us to provide our students with a quality education. target:\"\n",
    "\n",
    "out = generator(prompt, return_full_text=False, max_new_tokens=5, top_k=1)\n",
    "# [{'generated_text': ' contradiction'}]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
