{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02f76e55-3d57-41bb-97e4-46531fc68e3d",
   "metadata": {},
   "source": [
    "# Training PyTorch models on an IPU using Paperspace Gradient\n",
    "\n",
    "These notebooks are designed to help you solve the problems you face, by leveraging the power of deep learning and the speed of the Graphcore IPU. These examples use PopTorch, a set of extensions to PyTorch to enable PyTorch models to run on Graphcore's IPUs.\n",
    "\n",
    "Graph-structured data occurs in many problems dealing with complex systems of interacting entities. Applying machine learning methods to graph-structured data, in particular graph neural networks (GNNs), have seen a growth in popularity. These models are particularly well suited to the IPU architecture and we have a couple of worked examples you can use to learn how to solve your problems on the IPU.\n",
    "\n",
    "These examples will show you how to use and train PyTorch models which run on Graphcore IPUs using the Paperspace Gradient environment.  you will learn how to use an IPU in a Jupyter-style notebook and use it to train large real-world models including SchNet-GNN to predict molecular energy gaps and BERT-Large for natural language processing.\n",
    "\n",
    "The Paperspace environment lets you run these notebook with no special set up.\n",
    "\n",
    "## SchNet: graph neural network for quantum chemistry\n",
    "\n",
    "The [Prediction of Molecular Properties using SchNet](schnet-graph-network/PyG-SchNetGNN.ipynb) notebook demonstrates training a SchNet graph neural network with PyTorch Geometric (PyG) on the Graphcore IPU.\n",
    "\n",
    "This works through the example to show you how to get a large model running on the IPU and get the best performance out of it.\n",
    "\n",
    "## BERT-Large for natural language processing\n",
    "\n",
    "The [BERT-Large Fine Tuning](finetuning-bert/Fine-tuning-BERT.ipynb) notebook demonstrates how to fine-tune a pre-trained BERT model with PyTorch on the IPU. It uses a BERT-Large model and fine-tunes it on the SQuADv1 question answering task.\n",
    "\n",
    "## Temporal graph networks\n",
    "\n",
    "The notebook [Training Dynamic Graphs with Temporal Graph Networks](temporal-graph-networks/Train_TGN.ipynb) demonstrates how you can train a temporal graph networks (TGNs) on the IPU to predict connections in a dynamically evolving graph.\n",
    "\n",
    "You can read more about this in the blog post [Accelerating and Scaling Temporal Graph Networks on the Graphcore IPU](https://www.graphcore.ai/posts/accelerating-and-scaling-temporal-graph-networks-on-the-graphcore-ipu).\n",
    "\n",
    "## Visual transformer training on the IPU\n",
    "\n",
    "The tutorial [Training a Visual Transformer (ViT)](vit-model-training/walkthrough.ipynb) will walk you through the process of fine-tuning a Hugging face vision transformer (ViT) model to detect multiple diseases from chest X-rays. \n",
    "\n",
    "## Learning PyTorch \n",
    "\n",
    "The examples in the `learning-PyTorch-on-IPU` folder provide an introduction to using PopTorch to run PyTorch models on the IPU.\n",
    "\n",
    "* [Introduction to PopTorch](learning-PyTorch-on-IPU/basics/walkthrough.ipynb) introduces the basics of creating a model in PyTorch and then converting it to a PopTorch model to run on the IPU.\n",
    "* [Efficient Data Loading](learning-PyTorch-on-IPU/efficient_data_loading/walkthrough.ipynb) will show you how to perform efficient data transfer to the IPU, avoiding common performance bottlenecks.\n",
    "* [Half and Mixed Precision in PopTorch](learning-PyTorch-on-IPU/mixed_precision/walkthrough.ipynb) shows how you can make use of half-precision (16-bit) floating point to reduce data storage requirements and increase performance.\n",
    "* [Parallel Execution Using Pipelining](learning-PyTorch-on-IPU/pipelining/walkthrough.ipynb) demonstrates how to train and test a neural network by splitting the model over several IPUs and using pipelining to make efficient use of all the available IPUs.\n",
    "\n",
    "## Useful tips\n",
    "\n",
    "Finally, the `useful-tips` folder contains information about how to make best use of the IPU resources. For example monitoring IPU use, releasing IPUs when you are not using them, and then re-attaching your model to the IPU when you start again.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
