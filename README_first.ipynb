{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67c8a999",
   "metadata": {},
   "source": [
    "# Training PyTorch models on an IPU using Paperspace Gradient\n",
    "\n",
    "These notebooks are designed to help you solve the problems you face, by leveraging the power of deep learning and the speed of the Graphcore IPU. These examples use PopTorch, a set of extensions to PyTorch to enable PyTorch models to run on Graphcore's IPUs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35598d7e",
   "metadata": {},
   "source": [
    "<details><summary><big> Get started by learning how to use PyTorch</summary>\n",
    "\n",
    "The examples in the `learning-PyTorch-on-IPU` folder provide an introduction to using PopTorch to run PyTorch models on the IPU.\n",
    "    \n",
    "<ul style=“list-style-type:disc”>\n",
    "\n",
    "\n",
    "* [Introduction to PopTorch](learning-PyTorch-on-IPU/basics/walkthrough.ipynb) introduces the basics of creating a model in PyTorch and then converting it to a PopTorch model to run on the IPU.\n",
    "* [Efficient Data Loading](learning-PyTorch-on-IPU/efficient_data_loading/walkthrough.ipynb) will show you how to perform efficient data transfer to the IPU, avoiding common performance bottlenecks.\n",
    "* [Half and Mixed Precision in PopTorch](learning-PyTorch-on-IPU/mixed_precision/walkthrough.ipynb) shows how you can make use of half-precision (16-bit) floating point to reduce data storage requirements and increase performance.\n",
    "* [Parallel Execution Using Pipelining](learning-PyTorch-on-IPU/pipelining/walkthrough.ipynb) demonstrates how to train and test a neural network by splitting the model over several IPUs and using pipelining to make efficient use of all the available IPUs.\n",
    "    \n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4bb9f1",
   "metadata": {},
   "source": [
    "<details><summary><big> Explore our advanced ML examples</summary>\n",
    "    \n",
    "The [BERT-Large Fine Tuning](finetuning-bert/Fine-tuning-BERT.ipynb) notebook demonstrates how to fine-tune a pre-trained BERT model with PyTorch on the IPU. It uses a BERT-Large model and fine-tunes it on the SQuADv1 question answering task.\n",
    "    \n",
    "The tutorial [Training a Visual Transformer (ViT)](vit-model-training/walkthrough.ipynb) will walk you through the process of fine-tuning a Hugging face vision transformer (ViT) model to detect multiple diseases from chest X-rays. \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d974302",
   "metadata": {},
   "source": [
    "<details><summary><big> Use the IPU to train GNN's</summary>\n",
    "\n",
    "The [Knowledge Graph Embedding model notebook](/distributed-kge/KgeModelling.ipynb) will show you how to train a KGE model on the IPU using the ogbl-wikikg2 dataset. Check out our award winning results on this link prediction task for the [2022 Open Graph Benchmark Large Scale Challenge](https://ogb.stanford.edu/neurips2022/)\n",
    "    \n",
    "The [Prediction of Molecular Properties using SchNet](schnet-graph-network/PyG-SchNetGNN.ipynb) notebook demonstrates training a SchNet graph neural network with PyTorch Geometric (PyG) on the Graphcore IPU.    \n",
    "\n",
    "The notebook [Training Dynamic Graphs with Temporal Graph Networks](temporal-graph-networks/Train_TGN.ipynb) demonstrates how you can train a temporal graph networks (TGNs) on the IPU to predict connections in a dynamically evolving graph.\n",
    "    \n",
    "You can read more about this in the blog post [Accelerating and Scaling Temporal Graph Networks on the Graphcore IPU](https://www.graphcore.ai/posts/accelerating-and-scaling-temporal-graph-networks-on-the-graphcore-ipu).\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403ec8da",
   "metadata": {},
   "source": [
    "### Useful tips\n",
    "Finally, the [Managing IPU resources](useful-tips/managing_ipu_resources.ipynb) notebook contains information about how to make best use of the IPU resources. For example monitoring IPU use, releasing IPUs when you are not using them, and then re-attaching your model to the IPU when you start again."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
