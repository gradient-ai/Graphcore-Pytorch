{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67c8a999",
   "metadata": {},
   "source": [
    "# Training PyTorch models on an IPU using Paperspace Gradient\n",
    "\n",
    "These notebooks are designed to help you solve the problems you face, by leveraging the power of deep learning and the speed of the Graphcore IPU. These examples use PopTorch, a set of extensions to PyTorch to enable PyTorch models to run on Graphcore's IPUs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35598d7e",
   "metadata": {},
   "source": [
    "<details><summary><big> How to use PyTorch on the IPU</summary>\n",
    "\n",
    "The examples in the `learning-PyTorch-on-IPU` folder provide an introduction to using PopTorch to run PyTorch models on the IPU.\n",
    "    \n",
    "<ul style=“list-style-type:disc”>\n",
    "    \n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "<td align=\"left\">\n",
    "    <td> <img align=\"left\" width=\"18\" src=\"./images/folder_logo.png\" /> learning-PyTorch-on-IPU/ </td>\n",
    "    <td> </td\n",
    "</td>\n",
    "<tr>\n",
    "<td align=\"left\">\n",
    "    <td> &nbsp&nbsp&nbsp&nbsp <img align=\"left\" width=\"16\" src=\"./images/jupyter_logo.png\" /> basics/walkthrough.ipynb </td>\n",
    "    <td> introduces the basics of creating a model in PyTorch and then converting it to a PopTorch model to run on the IPU.\n",
    "    </td>\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td align=\"left\">\n",
    "    <td> &nbsp&nbsp&nbsp&nbsp <img align=\"left\" width=\"16\" src=\"./images/jupyter_logo.png\" /> efficient_data_loading/walkthrough.ipynb </td>\n",
    "    <td> will show you how to perform efficient data transfer to the IPU, avoiding common performance bottlenecks.\n",
    "    </td>\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td align=\"left\">\n",
    "    <td> &nbsp&nbsp&nbsp&nbsp <img align=\"left\" width=\"16\" src=\"./images/jupyter_logo.png\" /> mixed_precision/walkthrough.ipynb </td>\n",
    "    <td> shows how you can make use of half-precision (16-bit) floating point to reduce data storage requirements and increase performance.\n",
    "    </td>\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td align=\"left\">\n",
    "    <td> &nbsp&nbsp&nbsp&nbsp <img align=\"left\" width=\"16\" src=\"./images/jupyter_logo.png\" /> pipelining/walkthrough.ipynb </td>\n",
    "    <td> demonstrates how to train and test a neural network by splitting the model over several IPUs and using pipelining to make efficient use of all the available IPUs.\n",
    "    </td>\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4bb9f1",
   "metadata": {},
   "source": [
    "<details><summary><big> Explore our advanced fine-tuning notebooks</summary>\n",
    "    \n",
    "Fine-tune the BERT-Large model in our Natural-Language-Processing notebook, by running this notebook:\n",
    "    \n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "<td align=\"left\">\n",
    "    <td> <img align=\"left\" width=\"18\" src=\"./images/folder_logo.png\" /> finetuning-bert/ </td>\n",
    "    <td> </td\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td align=\"left\">\n",
    "    <td> &nbsp&nbsp&nbsp&nbsp <img align=\"left\" width=\"16\" src=\"./images/jupyter_logo.png\" /> Fine-tuning-BERT.ipynb </td>\n",
    "    <td> This notebook demonstrates how to fine-tune a pre-trained BERT-Large model on the SQuADv1 question answering task with PyTorch on the IPU.  </td\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "Fine-tune chest x-rays in our Vision Transformer (Vit) notebook:\n",
    "\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "<td align=\"left\">\n",
    "    <td> <img align=\"left\" width=\"18\" src=\"./images/folder_logo.png\" /> learning-PyTorch-on-IPU/ </td>\n",
    "    <td> </td\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td align=\"left\">\n",
    "    <td> &nbsp&nbsp&nbsp&nbsp <img align=\"left\" width=\"16\" src=\"./images/jupyter_logo.png\" /> vit-model-training/walkthrough.ipynb </td>\n",
    "    <td> This tutorial will walk you through the process of fine-tuning a Hugging face vision transformer (ViT) model to detect multiple diseases from chest X-rays.   </td\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d974302",
   "metadata": {},
   "source": [
    "<details><summary><big> Use the IPU to train GNN's</summary>\n",
    "\n",
    "Explore training GNN's for different tasks within the following notebooks:\n",
    "\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "<td align=\"left\">\n",
    "    <td> <img align=\"left\" width=\"18\" src=\"./images/folder_logo.png\" /> distributed-kge/ </td>\n",
    "    <td> </td\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td align=\"left\">\n",
    "    <td> &nbsp&nbsp&nbsp&nbsp <img align=\"left\" width=\"16\" src=\"./images/jupyter_logo.png\" /> KgeModelling.ipynb </td>\n",
    "    <td> The Knowledge Graph Embedding model notebook will show you how to train a KGE model on the IPU using the ogbl-wikikg2 dataset.</td>\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td align=\"left\">\n",
    "    <td> <img align=\"left\" width=\"18\" src=\"./images/folder_logo.png\" /> schnet-graph-network/ </td>\n",
    "    <td> </td\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td align=\"left\">\n",
    "    <td> &nbsp&nbsp&nbsp&nbsp <img align=\"left\" width=\"16\" src=\"./images/jupyter_logo.png\" /> PyG-SchNetGNN.ipynb </td>\n",
    "    <td> The Prediction of Molecular Properties using SchNet notebook demonstrates training a SchNet graph neural network with PyTorch Geometric (PyG) on the Graphcore IPU.    </td>\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td align=\"left\">\n",
    "    <td> <img align=\"left\" width=\"18\" src=\"./images/folder_logo.png\" /> temporal-graph-networks/ </td>\n",
    "    <td> </td\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td align=\"left\">\n",
    "    <td> &nbsp&nbsp&nbsp&nbsp <img align=\"left\" width=\"16\" src=\"./images/jupyter_logo.png\" /> Train_TGN.ipynb </td>\n",
    "    <td> The notebook Training Dynamic Graphs with Temporal Graph Networks demonstrates how you can train a temporal graph networks (TGNs) on the IPU to predict connections in a dynamically evolving graph. </td>\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "<br><br><br><br><br><br><br><br>\n",
    "    \n",
    "\n",
    "Check out our award winning results on the OGB link prediction task for the [2022 Open Graph Benchmark Large Scale Challenge](https://ogb.stanford.edu/neurips2022/).\n",
    "Read our blog, [Accelerating and Scaling Temporal Graph Networks on the Graphcore IPU](https://www.graphcore.ai/posts/accelerating-and-scaling-temporal-graph-networks-on-the-graphcore-ipu) to learn more about our TGN model.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403ec8da",
   "metadata": {},
   "source": [
    "### Useful tips\n",
    "Finally, the [Managing IPU resources](useful-tips/managing_ipu_resources.ipynb) notebook contains information about how to make best use of the IPU resources. For example monitoring IPU use, releasing IPUs when you are not using them, and then re-attaching your model to the IPU when you start again."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
