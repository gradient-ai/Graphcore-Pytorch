{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f6abe38",
   "metadata": {},
   "source": [
    "*Notebook autogenerated from gpt2_sentiment_analysis.py on 05-Dec-2022*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e986dff",
   "metadata": {},
   "source": [
    "# Sentiment analysis with GPT2 using IPUs\n",
    "\n",
    "\n",
    "The Generative Pre-trained Transformer 2 (GPT-2) model can be utilised for text generation, question answering, translation and summarisation. \n",
    "\n",
    "In this notebook, we will show you how to quickly utilise Pytorch and the IPU for fast sentiment analysis using our implementation of GPT-2 small and medium on an IPU-POD4, and GPT-2 Large on an IPU-POD16.\n",
    "\n",
    "To run fine-tuning on your own dataset with GPT-2 on IPU, please follow our tutorial on this [add ref]. \n",
    "\n",
    "First let's install our requirements and build our custom ops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5172767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt\n",
    "# !make"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9ba8b2",
   "metadata": {},
   "source": [
    "## Text Generation using GPT-2 Small\n",
    "\n",
    "GPT-2 Small is a 124M parameter model pre-trained on a vast corpus of English data, you can learn more about the model and the dataset used to train it on this [Hugging Face model card](https://huggingface.co/gpt2). \n",
    "\n",
    "Let's see how we can use the generative capabilities of GPT-2 to help us decide what to cook for dinner tonight with the following question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79d93f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_prompt = \"What should we cook for dinner tonight?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3b4c1f",
   "metadata": {},
   "source": [
    "GPT-2 Small has a small number of parameters meaning that it can be run on a single IPU, hence we must set options which are specific to work on this system configuration. \n",
    "Since we are not pipelining our model, we can set the majority of these settings to `None`. It is important to note here that:\n",
    " \n",
    " - `model_name_or_path` - allows us to refer to a specific GPT-2 model version, in this case we will be running inference on 'gpt2',\n",
    " - `single_ipu` - allows us to set specific defaults if running on only a single IPU, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5a765d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=1, device_iterations=1, fp16=True, input_len=150, layers_per_ipu=None, matmul_proportion=None, model_name_or_path='gpt2', output_len=256, prompt='What should we cook for dinner tonight?', repetition_penalty=2.0, save_samples_path=False, single_ipu=True, stop_token='#', temperature=1.2, tokenizer_type=0, topk=3, user_input=False)\n"
     ]
    }
   ],
   "source": [
    "from text_inference_pipeline import create_args\n",
    "\n",
    "args = create_args(\"gpt2\",\n",
    "                    single_ipu=True,\n",
    "                    layers_per_ipu=None,\n",
    "                    matmul_proportion=None,\n",
    "                    prompt = generation_prompt)\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148accd3",
   "metadata": {},
   "source": [
    "The `initialise_model` function loads and runs the model for one iteration on the input token, we can use that to see whats for dinner!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbd21639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:41:16.565] [poptorch::python] [warning] No device set in torch.randn(): forcing to IPU\n",
      "warning:poptorch::python:No device set in torch.randn(): forcing to IPU\n",
      "[15:41:16.567] [poptorch::python] [warning] No device set in torch.ones(): forcing to IPU\n",
      "warning:poptorch::python:No device set in torch.ones(): forcing to IPU\n",
      "Graph compilation: 100%|██████████| 100/100 [00:08<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What should we cook for dinner tonight? I'll tell ya. I've been trying to get some good, old fashioned recipes from the past, so this recipe is for me, but it will probably be the best one for you. I'm sure it will be a lot better for you, but I'll be sure not to disappoint. It is very simple and easy, so if you're not a huge vegetarian or vegan fan, I highly encourage that, because it's a great idea for a meal that will make a huge impact on the restorative properties in your diet and will help your immune function, and your body. It is a very good way to start, because it will give your food a little bit extra flavor and a lot of flavor to it that is very good for the digestive system and for the digestive health, but also helps your digestion, which will help to make your body better able and able for you to eat better. It is also a very simple recipe to make, so you don. I will try it out and I will share my recipe. It will be great. I'm sure you will enjoy this recipe.\n",
      "Posted By Anonymous at 10/11\n",
      "Posted By The_SpiritedAtlas on 10-10-2013\n",
      "This is my favorite dish of all time, but it's also the most delicious and delicious to eat, because it is the best way to eat. It's a very simple and delicious dish. I'm going to share this dish, because this recipe is so good and I think you will enjoy eating this. I will share this with my family, and we will be sharing it with you.\n",
      "I am a huge vegan. So, I have been trying to find the best vegan food recipe ever, but this is my first recipe and I am not going back to the original recipe because I have not been able, because it was not a vegan. I am not going into the details, but this recipe was made with a lot of dairy and soy\n"
     ]
    }
   ],
   "source": [
    "from text_inference_pipeline import initialise_model\n",
    "\n",
    "prompt, run_model, model, tokenizer = initialise_model(args)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e444f68f",
   "metadata": {},
   "source": [
    "From doing this small exercise, we have been able to see how quickly we can run and utilise GPT-2 to help us generate text!\n",
    "The generated text is quite random and without context the model has been unable to correctly identify the task which we are trying to complete.\n",
    "\n",
    "By focusing the task on Sentiment Analysis we should be able to  generate better results by providing context to the model through Few-Shot Learning.\n",
    "\n",
    "## Sentiment analysis using Few-Shot learning\n",
    "\n",
    "Using Few-Shot learning, we can guide GPT-2 to complete a more specific task by feeding the model context prior to the the input which we want the model to predict on.\n",
    "For sentiment analysis, we can structure sentences in the following format: \"Message: `text` Sentiment: `classification` ###\".\n",
    "\n",
    "The `text` will be a sentence with a `positive` , `negative` or `neutral` classification, the sentence finishes with `###` as the stop token.\n",
    "\n",
    "Using this structure, we will feed the model with the context it needs to predict the next token, let's use the following sentences for context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c60fb87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = \"Message: The weather has been horrible this winter... Sentiment: Negative ### \"\n",
    "pos = \"Message: I love the IPU, it is so fast! Sentiment: Positive ### \"\n",
    "neutral = \"Message: My family are coming to my house for dinner. Sentiment: Neutral ### \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8601c7",
   "metadata": {},
   "source": [
    "By concatenating these sentences together we have created the context needed to complete Few-Shot Learning on our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "218f2e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: The weather has been horrible this winter... Sentiment: Negative \n",
      "Message: I love the IPU, it is so fast! Sentiment: Positive \n",
      "Message: My family are coming to my house for dinner. Sentiment: Neutral \n",
      "\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = neg + pos + neutral\n",
    "print(few_shot_prompt.replace(\"### \",\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f01a06",
   "metadata": {},
   "source": [
    "Finally, we can now include a test prompt which we want to use to see if our model correctly predicts the next token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19b77f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: The weather has been horrible this winter... Sentiment: Negative \n",
      "Message: I love the IPU, it is so fast! Sentiment: Positive \n",
      "Message: My family are coming to my house for dinner. Sentiment: Neutral \n",
      "Message: That was the best movie I've seen this year! Sentiment:\n"
     ]
    }
   ],
   "source": [
    "test = \"Message: That was the best movie I've seen this year! Sentiment:\"\n",
    "print((few_shot_prompt + test).replace(\"### \",\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f4daf0",
   "metadata": {},
   "source": [
    "The context in `few_shot_prompt` will be used for used for additional experiments later on, hence we will keep this separate from the `test` prompt which we will use to initialise and test our model.\n",
    "\n",
    "# Fast inference on GPT-2 Small\n",
    "\n",
    "Now with our `few_shot_prompt` for context and our `test` prompt for inference we can utilise GPT-2 Small for fast inference on the ipu!\n",
    "\n",
    "Let's create a configuration to point towards these new prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6169f36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=1, device_iterations=1, fp16=True, input_len=150, layers_per_ipu=None, matmul_proportion=None, model_name_or_path='gpt2', output_len=256, prompt=\"Message: The weather has been horrible this winter... Sentiment: Negative ### Message: I love the IPU, it is so fast! Sentiment: Positive ### Message: My family are coming to my house for dinner. Sentiment: Neutral ### Message: That was the best movie I've seen this year! Sentiment:\", repetition_penalty=2.0, save_samples_path=False, single_ipu=True, stop_token='#', temperature=1.2, tokenizer_type=0, topk=3, user_input=False)\n"
     ]
    }
   ],
   "source": [
    "args_small = create_args(\"gpt2\",\n",
    "                         single_ipu=True,\n",
    "                         layers_per_ipu=None,\n",
    "                         matmul_proportion=None,\n",
    "                         prompt = few_shot_prompt + test)\n",
    "\n",
    "print(args_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb051372",
   "metadata": {},
   "source": [
    "The `initialise_model` function allows us to load and initialise the model on the IPU ready to quickly send the inputs from the host and receive the models outputs. \n",
    "\n",
    "Using this and the context which we put together earlier, we can now load our model and run sentiment analysis on our test prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81acfc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:42:03.749] [poptorch::python] [warning] No device set in torch.randn(): forcing to IPU\n",
      "warning:poptorch::python:No device set in torch.randn(): forcing to IPU\n",
      "[15:42:03.752] [poptorch::python] [warning] No device set in torch.ones(): forcing to IPU\n",
      "warning:poptorch::python:No device set in torch.ones(): forcing to IPU\n",
      "Graph compilation: 100%|██████████| 100/100 [00:08<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: The weather has been horrible this winter... Sentiment: Negative \n",
      "Message: I love the IPU, it is so fast! Sentiment: Positive \n",
      "Message: My family are coming to my house for dinner. Sentiment: Neutral \n",
      "Message: That was the best movie I've seen this year! Sentiment: Positive \n"
     ]
    }
   ],
   "source": [
    "from text_inference_pipeline import initialise_model\n",
    "# %% time\n",
    "prompt, run_small_model, small_model, small_tokenizer = initialise_model(args_small)\n",
    "print(prompt.replace(\"### \",\"\\n\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2226da6d",
   "metadata": {},
   "source": [
    "As we were hoping, our test token has been correctly classified to have a `Positive` sentiment!\n",
    "\n",
    "Now that we have successfully initialised our model and run sentiment analysis on our test prompt, we are ready to build a function which should allow us to correctly identify any inputs.\n",
    "\n",
    "The `sentiment_analysis` function below prompts the user to provide an input and fits that input into the correct format that we described above.\n",
    "To provide the model with context, we will use the `few_shot_prompt` that we defined earlier and concatenate that before the user input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a6cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(args, prompt, run_model, model, tokenizer):\n",
    "    user_input = \"### Message: \" + input() + \" Sentiment:\"\n",
    "    args.prompt = prompt + user_input\n",
    "    text_ids, txt_len, input_len = get_input(tokenizer, args)\n",
    "    model_output = run_model(text_ids, txt_len, model, tokenizer, input_len, args)\n",
    "    output = model_output[len(prompt):]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b65e13",
   "metadata": {},
   "source": [
    "Now we can feed our own inputs to the model to run sentiment analysis on the fly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369ca0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = sentiment_analysis(args_small, few_shot_prompt, run_small_model, small_model, small_tokenizer)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe322464",
   "metadata": {},
   "source": [
    "# Using GPT-2 Medium \n",
    "\n",
    "GPT-2 Medium has 355M parameters, and is the next size up from the GPT-2 model.\n",
    "This implementation has more decoder layers fitted to the model, which allows us to achieve more accurate results. \n",
    "\n",
    "Due to this increase in parameters, we must pipeline our model across 4 IPUs and set specific configuration options related to this larger model.\n",
    "To learn more about pipelining models on the IPU, see our [tutorial on this topic](https://github.com/graphcore/tutorials/tree/master/tutorials/pytorch/pipelining).\n",
    "\n",
    "We must set the following arguments to run GPT2 on 4 IPUs:\n",
    " - `model_name_or_path` must be changed to 'gpt2-medium',\n",
    " - `single_ipu` must now be set to `False`,\n",
    " - `layers_per_ipu` - specifies which of the model layers should be pipelined across the IPU, GPT-2 Medium is a 24 layer model which is split up across an IPU-POD4 with [1, 7, 8, 8] layers being placed on each chip respectively. \n",
    " - `matmul_proportion` - allows us to control how much temporary memory is used when doing matrix multiplication and convolution operations on each chip, to learn more read our docs on [Available Memory Proportion](https://docs.graphcore.ai/projects/available-memory/en/latest/index.html). Since we are running on a POD4 we can the memory proportion for each chip as shown below.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde6b34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_medium = create_args(model_name_or_path = 'gpt2-medium',\n",
    "                          single_ipu = False,\n",
    "                          layers_per_ipu = [1, 7, 8, 8],\n",
    "                          matmul_proportion = [0.2, 0.2, 0.2, 0.2],\n",
    "                          prompt = few_shot_prompt + test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62760c23",
   "metadata": {},
   "source": [
    "Now that we've set these configuration parameters, we are now ready to initialise a new model to prepare it for fast inference on the IPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df813d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt, run_medium_model, medium_model, medium_tokenizer = initialise_model(args_medium)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da359d6d",
   "metadata": {},
   "source": [
    "We can now run sentiment analysis on your own inputs using this model too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f0c6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = sentiment_analysis(args_medium, few_shot_prompt, run_medium_model, medium_model, medium_tokenizer)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5466f5b",
   "metadata": {},
   "source": [
    "# Using GPT2-Large on an IPU-POD16\n",
    "\n",
    "GPT 2 Large is a 36 layer model which has 774M parameters, and should provide you with even better results than the previous two models. \n",
    "If you have access to an IPU-POD16, you can push the abilities of GPT2 further by running inference our implementation of GPT2-Large.\n",
    "\n",
    "Since this is a much larger implementation, we must pipeline the 36 layers in the model across 16 IPUs in order to fit them within the memory constraints of the IPU, as well as tuning the `matmul_proportion` constraint.\n",
    "The arguments below allow us to reset these parameters to fit this new configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2d04ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_large = create_args(model_name_or_path = 'gpt2-large',\n",
    "                          single_ipu = False,\n",
    "                          layers_per_ipu = [0, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2],\n",
    "                          matmul_proportion = [0.2, 0.15, 0.2, 0.2, 0.2, 0.15, 0.15, 0.2, 0.2, 0.15, 0.2, 0.2, 0.2, 0.15, 0.15, 0.2],\n",
    "                          prompt = few_shot_prompt + test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75ea44f",
   "metadata": {},
   "source": [
    "Again, in order to prepare the model ready for inference, we must initialising this model again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daafa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt, run_model, model, tokenizer = initialise_model(args_large)\n",
    "\n",
    "prompt, output = sentiment_analysis(args_large, few_shot_prompt, run_model, model, tokenizer)\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6db615d",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook we have seen how to quickly and easily use GPT-2 to run inference on user inputs for sentiment analysis!\n",
    "We have also seen how to configure the IPU when scaling up our model up to larger GPT-2 implementations.\n",
    "\n",
    "Detach!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "traceability": {
   "sdk_version": "3.0.0+1145",
   "source_file": "gpt2_sentiment_analysis.py",
   "sst_version": "0.0.10",
   "timestamp": "2022-12-05T15:34"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
